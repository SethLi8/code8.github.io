<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Pyspark-SparkSQL | 靓仔阿胜的博客</title><meta name="author" content="靓仔阿胜"><meta name="copyright" content="靓仔阿胜"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="ffffff"><meta name="description" content="3.PySpark-SparkSQL 在Spark上运用SQL处理结构化数据  3.1 SparkSQL快速入门3.1.1 什么是SparkSQL SparkSQL是Spark的一个模块，用于处理海量结构化数据 限定：结构化数据处理 3.1.2 为什么要学习SparkSQL SparkSQL是非常成熟的 海量结构化数据处理框架。 学习SparkSQL主要在2个点:  SparkSQL本身十分优秀,">
<meta property="og:type" content="article">
<meta property="og:title" content="Pyspark-SparkSQL">
<meta property="og:url" content="https://sql88.github.io/post/1e5a3fd5.html">
<meta property="og:site_name" content="靓仔阿胜的博客">
<meta property="og:description" content="3.PySpark-SparkSQL 在Spark上运用SQL处理结构化数据  3.1 SparkSQL快速入门3.1.1 什么是SparkSQL SparkSQL是Spark的一个模块，用于处理海量结构化数据 限定：结构化数据处理 3.1.2 为什么要学习SparkSQL SparkSQL是非常成熟的 海量结构化数据处理框架。 学习SparkSQL主要在2个点:  SparkSQL本身十分优秀,">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://sql88.github.io/img/avatar1.jpg">
<meta property="article:published_time" content="2023-08-19T09:22:00.000Z">
<meta property="article:modified_time" content="2023-11-10T07:02:53.977Z">
<meta property="article:author" content="靓仔阿胜">
<meta property="article:tag" content="SparkSQL">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://sql88.github.io/img/avatar1.jpg"><link rel="shortcut icon" href="/img/as4.png"><link rel="canonical" href="https://sql88.github.io/post/1e5a3fd5.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":500,"languages":{"author":"作者: 靓仔阿胜","link":"链接: ","source":"来源: 靓仔阿胜的博客","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Pyspark-SparkSQL',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-11-10 15:02:53'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/avatar1.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">45</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">37</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">21</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 生活</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/shuoshuo/"><i class="fa-fw fa fa-comments-o"></i><span> 分享</span></a></li><li><a class="site-page child" href="/photos/"><i class="fa-fw fa fa-camera-retro"></i><span> 相册</span></a></li><li><a class="site-page child" href="/music/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/comment/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于笔者</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/img/background1.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="靓仔阿胜的博客"><span class="site-name">靓仔阿胜的博客</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 生活</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/shuoshuo/"><i class="fa-fw fa fa-comments-o"></i><span> 分享</span></a></li><li><a class="site-page child" href="/photos/"><i class="fa-fw fa fa-camera-retro"></i><span> 相册</span></a></li><li><a class="site-page child" href="/music/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/comment/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于笔者</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Pyspark-SparkSQL</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-08-19T09:22:00.000Z" title="发表于 2023-08-19 17:22:00">2023-08-19</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-11-10T07:02:53.977Z" title="更新于 2023-11-10 15:02:53">2023-11-10</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/">Spark</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">12.7k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>56分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Pyspark-SparkSQL"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="3-PySpark-SparkSQL"><a href="#3-PySpark-SparkSQL" class="headerlink" title="3.PySpark-SparkSQL"></a>3.PySpark-SparkSQL</h1><blockquote>
<p>在Spark上运用SQL处理结构化数据</p>
</blockquote>
<h2 id="3-1-SparkSQL快速入门"><a href="#3-1-SparkSQL快速入门" class="headerlink" title="3.1 SparkSQL快速入门"></a>3.1 SparkSQL快速入门</h2><h3 id="3-1-1-什么是SparkSQL"><a href="#3-1-1-什么是SparkSQL" class="headerlink" title="3.1.1 什么是SparkSQL"></a>3.1.1 什么是SparkSQL</h3><p><img src="https://cdn.jsdelivr.net/gh/Sql88/BlogImg@main/img/image-20230819174120604.png" alt="image-20230819174120604"></p>
<p>SparkSQL是Spark的一个模块，用于<font color='red' size=4>处理海量结构化数据</font></p>
<p>限定：结构化数据处理</p>
<h3 id="3-1-2-为什么要学习SparkSQL"><a href="#3-1-2-为什么要学习SparkSQL" class="headerlink" title="3.1.2 为什么要学习SparkSQL"></a>3.1.2 为什么要学习SparkSQL</h3><p><img src="https://cdn.jsdelivr.net/gh/Sql88/BlogImg@main/img/image-20230819173142314.png" alt="image-20230819173142314"></p>
<p>SparkSQL是非常成熟的 海量结构化数据处理框架。</p>
<p>学习SparkSQL主要在2个点:</p>
<ol>
<li>SparkSQL本身十分优秀, 支持SQL语言、性能强、可以自动优化、API简单、兼容HIVE等等</li>
<li>企业大面积在使用SparkSQL处理业务数据</li>
</ol>
<ul>
<li><pre><code>​     离线开发
</code></pre>
</li>
<li>​	 数仓搭建</li>
<li>​	 科学计算</li>
<li>​	 数据分析</li>
</ul>
<h3 id="3-1-3-SparkSQL特点"><a href="#3-1-3-SparkSQL特点" class="headerlink" title="3.1.3 SparkSQL特点"></a>3.1.3 SparkSQL特点</h3><p><img src="https://cdn.jsdelivr.net/gh/Sql88/BlogImg@main/img/image-20230819174237330.png" alt="image-20230819174237330"></p>
<h3 id="3-1-4-SparkSQL发展史"><a href="#3-1-4-SparkSQL发展史" class="headerlink" title="3.1.4 SparkSQL发展史"></a>3.1.4 SparkSQL发展史</h3><p><img src="https://cdn.jsdelivr.net/gh/Sql88/BlogImg@main/img/image-20230819174207223.png" alt="image-20230819174207223"></p>
<p>在许多年前(2012\2013左右)Hive逐步火热起来, 大片抢占分布式SQL计算市场</p>
<p>Spark作为通用计算框架, 也不可能放弃这一细分领域.</p>
<p>于是, Spark官方模仿Hive推出了Shark框架(Spark 0.9版本)</p>
<p>Shark框架是几乎100%模仿Hive, 内部的配置项\优化项等都是直接模仿而来.</p>
<p>不同点在于将执行引擎由MapReduce更换为了Spark.</p>
<p>因为Shark框架太模仿Hive, Hive是针对MR优化, 很多地方和SparkCore(RDD)水土不服, 最终被放弃</p>
<p>Spark官方下决心开发一个自己的分布式SQL引擎 也就是诞生了现在的SparkSQL</p>
<hr>
<p><img src="https://cdn.jsdelivr.net/gh/Sql88/BlogImg@main/img/image-20230819173814501.png" alt="image-20230819173814501"></p>
<p>● 2014年 1.0正式发布</p>
<p>● 2015年 1.3 发布DataFrame数据结构, 沿用至今</p>
<p>● 2016年 1.6 发布Dataset数据结构(带泛型的DataFrame), 适用于支持泛型的语言(Java\Scala)</p>
<p>● 2016年 2.0 统一了Dataset 和 DataFrame, 以后只有Dataset了, Python用的DataFrame就是 没有泛型的Dataset</p>
<p>● 2019年 3.0 发布， 性能大幅度提升，SparkSQL变化不大</p>
<h3 id="3-1-5-总结"><a href="#3-1-5-总结" class="headerlink" title="3.1.5 总结"></a>3.1.5 总结</h3><blockquote>
<ol>
<li><p>SparkSQL用于处理大规模结构化数据的计算引擎</p>
</li>
<li><p>SparkSQL在企业中广泛使用，并性能极好，学习它不管是工作还是就业都有很大帮助</p>
</li>
<li><p>SparkSQL：使用简单、API统一、兼容HIVE、支持标准化JDBC和ODBC连接</p>
</li>
<li><p>SparkSQL 2014年正式发布，当下使用最多的2.0版，Spark发布于2016年，当下使用的最新3.0办发布于2019年</p>
</li>
</ol>
</blockquote>
<h2 id="3-2-SparkSQL概述"><a href="#3-2-SparkSQL概述" class="headerlink" title="3.2 SparkSQL概述"></a>3.2 SparkSQL概述</h2><h3 id="3-2-1-SparkSQL和Hive的异同"><a href="#3-2-1-SparkSQL和Hive的异同" class="headerlink" title="3.2.1 SparkSQL和Hive的异同"></a>3.2.1 SparkSQL和Hive的异同</h3><p><img src="https://cdn.jsdelivr.net/gh/Sql88/BlogImg@main/img/image-20230819190806954.png" alt="image-20230819190806954"></p>
<ul>
<li><strong>Hive和Spark均是：“分布式SQL计算引擎”</strong></li>
<li><strong>均是构建大规模结构化数据计算的绝佳利器，同时SparkSQL拥有更好的性能。</strong></li>
<li><strong>目前，企业中使用Hive仍旧居多，但SparkSQL将会在很近的未来替代Hive成为分布式SQL计算市场的顶级</strong></li>
</ul>
<h3 id="3-2-2-SparkSQL的数据抽象"><a href="#3-2-2-SparkSQL的数据抽象" class="headerlink" title="3.2.2 SparkSQL的数据抽象"></a>3.2.2 SparkSQL的数据抽象</h3><p><img src="https://cdn.jsdelivr.net/gh/Sql88/BlogImg@main/img/image-20230819191543792.png" alt="image-20230819191543792"></p>
<hr>
<p><img src="https://cdn.jsdelivr.net/gh/Sql88/BlogImg@main/img/image-20230819191526430.png" alt="image-20230819191526430"></p>
<h3 id="3-2-3-SparkSQL数据抽象的发展"><a href="#3-2-3-SparkSQL数据抽象的发展" class="headerlink" title="3.2.3 SparkSQL数据抽象的发展"></a>3.2.3 SparkSQL数据抽象的发展</h3><p><img src="https://cdn.jsdelivr.net/gh/Sql88/BlogImg@main/img/image-20230819191713814.png" alt="image-20230819191713814"></p>
<blockquote>
<p>从SparkSQL的发展历史可以看到：</p>
<p>14年最早的数据抽象是：SchemaRDD（内部存储二维表数据结构的RDD），SchemaRDD就是魔改的RDD，将RDD支持的存储数据，限定</p>
<p>为二维表数据结构用以支持SQL查询。由于是魔改RDD，只是一个过渡产品，现已废弃。</p>
<p>15年发布DataFrame对象，基于Pandas的DataFrame（模仿）独立于RDD进行实现，将数据以二维表结构进行存储并支持分布式运行</p>
<p>16年发布DataSet对象，在DataFrame之上添加了泛型的支持，用以更好的支持Java和Scala这两个支持泛型的编程语言</p>
<p>16年，Spark2.0版本，将DataFrame和DataSet进行合并。其底层均是DataSet对象，但在Python和R语言到用时，显示为DataFrame对象。和老的DataFrame对象没有区别</p>
</blockquote>
<h3 id="3-2-4-DataFrame概述"><a href="#3-2-4-DataFrame概述" class="headerlink" title="3.2.4 DataFrame概述"></a>3.2.4 DataFrame概述</h3><p>DataFrame和RDD都是：弹性的、分布式的、数据集</p>
<p>只是，DataFrame存储的数据结构“限定”为：<code>二维表结构化数据</code></p>
<p>而RDD可以存储的数据则<code>没有任何限制</code>，想处理什么就处理什么</p>
<p><img src="https://cdn.jsdelivr.net/gh/Sql88/BlogImg@main/img/image-20230819192349796.png" alt="image-20230819192349796"></p>
<hr>
<p>假定有如下数据集：</p>
<p><img src="https://cdn.jsdelivr.net/gh/Sql88/BlogImg@main/img/image-20230819193133157.png" alt="image-20230819193133157"></p>
<p><strong>DataFrame按二维表存储：</strong></p>
<p>​	<img src="https://cdn.jsdelivr.net/gh/Sql88/BlogImg@main/img/image-20230819193144162.png" alt="image-20230819193144162"></p>
<p><strong>RDD按数组对象存储：</strong><br>            <img src="https://cdn.jsdelivr.net/gh/Sql88/BlogImg@main/img/image-20230819193159002.png" alt="image-20230819193159002"></p>
<ul>
<li>DataFrame 是按照二维表格的形式存储数据</li>
<li>RDD则是存储对象本身</li>
</ul>
<h3 id="3-2-5-SparkSession对象"><a href="#3-2-5-SparkSession对象" class="headerlink" title="3.2.5 SparkSession对象"></a>3.2.5 SparkSession对象</h3><p>在RDD阶段，程序的执行入口对象是： SparkContext</p>
<p>在Spark 2.0后，推出了SparkSession对象，作为Spark编码的统一入口对象。</p>
<p>SparkSession对象可以：</p>
<ul>
<li><p>用于SparkSQL编程作为入口对象</p>
</li>
<li><p>用于SparkCore编程，可以通过SparkSession对象中获取到SparkContext</p>
<p><font color='red'>所以，我们后续的代码，执行环境入口对象，统一变更为SparkSession对象</font></p>
<p><img src="https://cdn.jsdelivr.net/gh/Sql88/BlogImg@main/img/image-20230819193053095.png" alt="image-20230819193053095"></p>
<hr>
<p><font color='red'>现在，来体验一下构建执行环境入口对象：SparkSession</font></p>
<p><font color='red'>构建SparkSession核心代码</font></p>
<p><img src="https://cdn.jsdelivr.net/gh/Sql88/BlogImg@main/img/image-20230819193407862.png" alt="image-20230819193407862"></p>
<p>应该是.master(“local[*]”)，见下文代码</p>
</li>
</ul>
<h3 id="3-2-6-SparkSQL-HelloWorld"><a href="#3-2-6-SparkSQL-HelloWorld" class="headerlink" title="3.2.6 SparkSQL HelloWorld"></a>3.2.6 SparkSQL HelloWorld</h3><p>需求：读取文件，找出学科为“语文”的数据，并限制输出5条</p>
<p>where subject &#x3D; ‘语文’ limit 5</p>
<p>代码演示：<code>00_spark_session_create.py</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding:utf8</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># SparkSession对象的导包，对象是来自于pyspark.sql包中</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># 构建SparkSession执行环境入口对象</span></span><br><span class="line">    spark = SparkSession.builder. \</span><br><span class="line">        appName(<span class="string">&quot;00_spark_session_create.py&quot;</span>). \</span><br><span class="line">        master(<span class="string">&quot;local[*]&quot;</span>). \</span><br><span class="line">        getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 通过SparkSession对象，获取SparkContext对象</span></span><br><span class="line">    sc = spark.sparkContext</span><br><span class="line"></span><br><span class="line">    <span class="comment"># SparkSQL的HelloWorld</span></span><br><span class="line">    df = spark.read.csv(<span class="string">&quot;../data/input/stu_score.txt&quot;</span>, sep=<span class="string">&#x27;,&#x27;</span>, header=<span class="literal">False</span>)</span><br><span class="line">    df2 = df.toDF(<span class="string">&quot;id&quot;</span>, <span class="string">&quot;name&quot;</span>, <span class="string">&quot;score&quot;</span>)</span><br><span class="line">    df2.printSchema() <span class="comment"># 打印表结构</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;读取并打印数据&quot;</span>)</span><br><span class="line">    df2.show() <span class="comment"># 显示数据</span></span><br><span class="line"></span><br><span class="line">    df2.createTempView(<span class="string">&quot;score&quot;</span>) <span class="comment"># 构建score表</span></span><br><span class="line">    <span class="comment"># SQL风格</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;SQL 风格结果:&quot;</span>)</span><br><span class="line">    spark.sql(<span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        select * from score where name =&#x27;语文&#x27; limit 5</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span>).show()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># DSL 风格</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;DSL 风格结果:&quot;</span>)</span><br><span class="line">    df2.where(<span class="string">&quot;name=&#x27;语文&#x27;&quot;</span>).limit(<span class="number">5</span>).show()</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>输出结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">root</span><br><span class="line"> |-- id: string (nullable = true)</span><br><span class="line"> |-- name: string (nullable = true)</span><br><span class="line"> |-- score: string (nullable = true)</span><br><span class="line"></span><br><span class="line">读取并打印数据</span><br><span class="line">+---+----+-----+</span><br><span class="line">| id|name|score|</span><br><span class="line">+---+----+-----+</span><br><span class="line">|  1|语文|   99|</span><br><span class="line">|  2|语文|   99|</span><br><span class="line">|  3|语文|   99|</span><br><span class="line">|  4|语文|   99|</span><br><span class="line">|  5|语文|   99|</span><br><span class="line">|  6|语文|   99|</span><br><span class="line">|  7|语文|   99|</span><br><span class="line">|  8|语文|   99|</span><br><span class="line">|  9|语文|   99|</span><br><span class="line">| 10|语文|   99|</span><br><span class="line">| 11|语文|   99|</span><br><span class="line">| 12|语文|   99|</span><br><span class="line">| 13|语文|   99|</span><br><span class="line">| 14|语文|   99|</span><br><span class="line">| 15|语文|   99|</span><br><span class="line">| 16|语文|   99|</span><br><span class="line">| 17|语文|   99|</span><br><span class="line">| 18|语文|   99|</span><br><span class="line">| 19|语文|   99|</span><br><span class="line">| 20|语文|   99|</span><br><span class="line">+---+----+-----+</span><br><span class="line">only showing top 20 rows</span><br><span class="line"></span><br><span class="line">SQL 风格结果:</span><br><span class="line">+---+----+-----+</span><br><span class="line">| id|name|score|</span><br><span class="line">+---+----+-----+</span><br><span class="line">|  1|语文|   99|</span><br><span class="line">|  2|语文|   99|</span><br><span class="line">|  3|语文|   99|</span><br><span class="line">|  4|语文|   99|</span><br><span class="line">|  5|语文|   99|</span><br><span class="line">+---+----+-----+</span><br><span class="line"></span><br><span class="line">DSL 风格结果:</span><br><span class="line">+---+----+-----+</span><br><span class="line">| id|name|score|</span><br><span class="line">+---+----+-----+</span><br><span class="line">|  1|语文|   99|</span><br><span class="line">|  2|语文|   99|</span><br><span class="line">|  3|语文|   99|</span><br><span class="line">|  4|语文|   99|</span><br><span class="line">|  5|语文|   99|</span><br><span class="line">+---+----+-----+</span><br></pre></td></tr></table></figure>

<h3 id="3-2-7-总结"><a href="#3-2-7-总结" class="headerlink" title="3.2.7 总结"></a>3.2.7 总结</h3><blockquote>
<ol>
<li><p>SparkSQL 和 Hive同样，都是用于大规模SQL分布式计算的计算框架，均可以运行在YARN之上，在企业中广泛被应用</p>
</li>
<li><p>SparkSQL的数据抽象为：SchemaRDD（废弃）、DataFrame（Python、R、Java、Scala）、DataSet（Java、Scala）。</p>
</li>
<li><p>DataFrame同样是分布式数据集，有分区可以并行计算，和RDD不同的是，DataFrame中存储的数据结构是以表格形式组织的，方便进行SQL计算</p>
</li>
<li><p>DataFrame对比DataSet基本相同，不同的是DataSet支持泛型特性，可以让Java、Scala语言更好的利用到。</p>
</li>
<li><p>SparkSession是2.0后推出的新执行环境入口对象，可以用于RDD、SQL等编程</p>
</li>
</ol>
</blockquote>
<h2 id="3-3-DataFrame入门"><a href="#3-3-DataFrame入门" class="headerlink" title="3.3 DataFrame入门"></a>3.3 DataFrame入门</h2><h3 id="3-3-1-DataFrame的组成"><a href="#3-3-1-DataFrame的组成" class="headerlink" title="3.3.1 DataFrame的组成"></a>3.3.1 DataFrame的组成</h3><p>DataFrame是一个二维表结构， 那么表格结构就有无法绕开的三个点：</p>
<ul>
<li>行</li>
<li>列</li>
<li>表结构表述</li>
</ul>
<p>比如，在MySQL中的一张表：</p>
<ul>
<li><p>由许多行组成</p>
</li>
<li><p>数据也被分成多个列</p>
</li>
<li><p>表也有表结构信息（列、列名、列类型、列约束等）</p>
</li>
</ul>
<p>基于这个前提，DataFrame的组成如下：</p>
<p>在结构层面：</p>
<ul>
<li><code>StructType</code>对象描述整个DataFrame的表结构</li>
<li><code>StructField</code>对象描述一个列的信息</li>
</ul>
<p>在数据层面：</p>
<ul>
<li><code>Row</code>对象记录一行数据</li>
<li><code>Column</code>对象记录一列数据并包含列的信息</li>
</ul>
<hr>
<p><img src="https://cdn.jsdelivr.net/gh/Sql88/BlogImg@main/img/image-20230824222446129.png" alt="image-20230824222446129"></p>
<p>如图， 在表结构层面，DataFrame的表结构由StructType描述，如下图：</p>
<p><img src="https://cdn.jsdelivr.net/gh/Sql88/BlogImg@main/img/image-20230824222534304.png" alt="image-20230824222534304"></p>
<p>一个<code>StructField</code>记录：列名、列类型、列是否运行为空</p>
<p>多个<code>StructField</code>组成一个<code>StructType</code>对象。</p>
<p>一个<code>StructType</code>对象可以描述一个DataFrame：有几个列、每个列的名字和类型、每个列是否为空</p>
<p>同时，一行数据描述为<code>Row</code>对象，如<code>Row</code>(1, 张三, 11)</p>
<p>一列数据描述为<code>Column</code>对象，<code>Column</code>对象包含一列数据和列的信息</p>
<p>Row、Column、StructType、StructField的编程我们在后面编码阶段会接触</p>
<h3 id="3-3-2-DataFrame的代码构建"><a href="#3-3-2-DataFrame的代码构建" class="headerlink" title="3.3.2 DataFrame的代码构建"></a>3.3.2 DataFrame的代码构建</h3><h4 id="3-3-2-1-基于RDD方式1"><a href="#3-3-2-1-基于RDD方式1" class="headerlink" title="3.3.2.1 基于RDD方式1"></a>3.3.2.1 基于RDD方式1</h4><p>DataFrame对象可以从RDD转换而来，都是分布式数据集，其实就是转换一下内部存储的结构，转换为二维表结构</p>
<p>将RDD转换为DataFrame方式1：</p>
<p>调用spark</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 首先构建一个RDD rdd[(name, age), ()]</span></span><br><span class="line">rdd = sc.textFile(<span class="string">&quot;../data/sql/people.txt&quot;</span>).\</span><br><span class="line"><span class="built_in">map</span>(<span class="keyword">lambda</span> x: x.split(<span class="string">&#x27;,&#x27;</span>)).\</span><br><span class="line"><span class="built_in">map</span>(<span class="keyword">lambda</span> x: [x[<span class="number">0</span>], <span class="built_in">int</span>(x[<span class="number">1</span>])]) <span class="comment"># 需要做类型转换, 因为类型从RDD中探测</span></span><br><span class="line"><span class="comment"># 构建DF方式1</span></span><br><span class="line">df = spark.createDataFrame(rdd, schema = [<span class="string">&#x27;name&#x27;</span>, <span class="string">&#x27;age&#x27;</span>])</span><br></pre></td></tr></table></figure>

<p>通过<code>SparkSession</code>对象的<code>createDataFrame</code>方法来将RDD转换为DataFrame</p>
<p>这里只传入列名称，类型从RDD中进行推断，是否允许为空，默认为允许（True）</p>
<p>完整代码演示：<code>01_dataframe_create1.py</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding:utf8</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># 0. 构建执行环境入口对象SparkSession</span></span><br><span class="line">    spark = SparkSession.builder. \</span><br><span class="line">        appName(<span class="string">&quot;01_dataframe_create1.py&quot;</span>). \</span><br><span class="line">        master(<span class="string">&quot;local[*]&quot;</span>). \</span><br><span class="line">        getOrCreate()</span><br><span class="line"></span><br><span class="line">    sc = spark.sparkContext</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 1.基于RDD转换成DataFrame</span></span><br><span class="line">    rdd = sc.textFile(<span class="string">&quot;../data/input/sql/people.txt&quot;</span>). \</span><br><span class="line">        <span class="built_in">map</span>(<span class="keyword">lambda</span> x: x.split(<span class="string">&quot;,&quot;</span>)). \</span><br><span class="line">        <span class="built_in">map</span>(<span class="keyword">lambda</span> x: (x[<span class="number">0</span>], <span class="built_in">int</span>(x[<span class="number">1</span>]))) <span class="comment"># 需要做类型转换, 因为类型从RDD中探测</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 2.构建DataFrame对象</span></span><br><span class="line">    <span class="comment"># 参数1 被转换的RDD</span></span><br><span class="line">    <span class="comment"># 参数2 指定列名，通过list的形式指定，按照顺序依次提供字符串名称即可</span></span><br><span class="line">    df = spark.createDataFrame(rdd, schema=[<span class="string">&#x27;name&#x27;</span>, <span class="string">&#x27;age&#x27;</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 打印DataFrame的表结构</span></span><br><span class="line">    df.printSchema()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 打印df中的数据</span></span><br><span class="line">    <span class="comment"># 参数1 表示展示出多少条数据，默认不传的话是20</span></span><br><span class="line">    <span class="comment"># 参数2 表示是否对列进行截断，如果列的数据长度超过20个字符串长度，后续的内容不显示，以...代替</span></span><br><span class="line">    <span class="comment"># 如果给False 表示不截断，全部显示，默认是True</span></span><br><span class="line">    df.show(<span class="number">20</span>, <span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将DF对象转换成临时视图表，可供sql语句查询</span></span><br><span class="line">    df.createOrReplaceTempView(<span class="string">&quot;people&quot;</span>)</span><br><span class="line">    spark.sql(<span class="string">&quot;select * from people where age &lt; 30&quot;</span>).show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>输出结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">root</span><br><span class="line"> |-- name: string (nullable = true)</span><br><span class="line"> |-- age: long (nullable = true)</span><br><span class="line"></span><br><span class="line">+-------+---+</span><br><span class="line">|name   |age|</span><br><span class="line">+-------+---+</span><br><span class="line">|Michael|29 |</span><br><span class="line">|Andy   |30 |</span><br><span class="line">|Justin |19 |</span><br><span class="line">+-------+---+</span><br><span class="line"></span><br><span class="line">+-------+---+</span><br><span class="line">|   name|age|</span><br><span class="line">+-------+---+</span><br><span class="line">|Michael| 29|</span><br><span class="line">| Justin| 19|</span><br><span class="line">+-------+---+</span><br></pre></td></tr></table></figure>

<h4 id="3-3-2-2基于RDD方式2"><a href="#3-3-2-2基于RDD方式2" class="headerlink" title="3.3.2.2基于RDD方式2"></a>3.3.2.2基于RDD方式2</h4><p>将RDD转换为DataFrame方式2：</p>
<p>​	通过<code>StructType</code>对象来定义DataFrame的“表结构”转换RDD</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建DF , 首先创建RDD 将RDD转DF</span></span><br><span class="line">rdd = sc.textFile(<span class="string">&quot;../data/sql/stu_score.txt&quot;</span>).\</span><br><span class="line"><span class="built_in">map</span>(<span class="keyword">lambda</span> x:x.split(<span class="string">&#x27;,&#x27;</span>)).\</span><br><span class="line"><span class="built_in">map</span>(<span class="keyword">lambda</span> x:(<span class="built_in">int</span>(x[<span class="number">0</span>]), x[<span class="number">1</span>], <span class="built_in">int</span>(x[<span class="number">2</span>])))</span><br><span class="line"><span class="comment"># StructType 类</span></span><br><span class="line"><span class="comment"># 这个类 可以定义整个DataFrame中的Schema</span></span><br><span class="line">schema = StructType().\</span><br><span class="line">add(<span class="string">&quot;id&quot;</span>, IntegerType(), nullable=<span class="literal">False</span>).\</span><br><span class="line">add(<span class="string">&quot;name&quot;</span>, StringType(), nullable=<span class="literal">True</span>).\</span><br><span class="line">add(<span class="string">&quot;score&quot;</span>, IntegerType(), nullable=<span class="literal">False</span>)</span><br><span class="line"><span class="comment"># 一个add方法 定义一个列的信息, 如果有3个列, 就写三个add, 每一个add代表一个StructField</span></span><br><span class="line"><span class="comment"># add方法: 参数1: 列名称, 参数2: 列类型, 参数3: 是否允许为空</span></span><br><span class="line">df = spark.createDataFrame(rdd, schema)</span><br></pre></td></tr></table></figure>

<p>完整代码演示：<code>02_dataframe_create2.py</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding:utf8</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> StructType, StringType, IntegerType</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># 0. 构建执行环境入口对象SparkSession</span></span><br><span class="line">    spark = SparkSession.builder. \</span><br><span class="line">        appName(<span class="string">&quot;02_dataframe_create2.py&quot;</span>). \</span><br><span class="line">        master(<span class="string">&quot;local[*]&quot;</span>). \</span><br><span class="line">        getOrCreate()</span><br><span class="line"></span><br><span class="line">    sc = spark.sparkContext</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 基于RDD转换成DataFrame</span></span><br><span class="line">    rdd = sc.textFile(<span class="string">&quot;../data/input/sql/people.txt&quot;</span>). \</span><br><span class="line">        <span class="built_in">map</span>(<span class="keyword">lambda</span> x: x.split(<span class="string">&quot;,&quot;</span>)). \</span><br><span class="line">        <span class="built_in">map</span>(<span class="keyword">lambda</span> x: (x[<span class="number">0</span>], <span class="built_in">int</span>(x[<span class="number">1</span>])))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 构建表结构的描述对象：StructType对象:StructType对象</span></span><br><span class="line">    schema = StructType().add(<span class="string">&quot;name&quot;</span>,StringType(),nullable=<span class="literal">True</span>).\</span><br><span class="line">        add(<span class="string">&quot;age&quot;</span>,IntegerType(),nullable=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 基于StructType对象去构建RDD到DF的转换</span></span><br><span class="line">    df = spark.createDataFrame(rdd,schema=schema)</span><br><span class="line"></span><br><span class="line">    df.printSchema()</span><br><span class="line">    df.show()</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>输出结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">root</span><br><span class="line"> |-- name: string (nullable = true)</span><br><span class="line"> |-- age: integer (nullable = false)</span><br><span class="line"></span><br><span class="line">+-------+---+</span><br><span class="line">|   name|age|</span><br><span class="line">+-------+---+</span><br><span class="line">|Michael| 29|</span><br><span class="line">|   Andy| 30|</span><br><span class="line">| Justin| 19|</span><br><span class="line">+-------+---+</span><br></pre></td></tr></table></figure>

<h4 id="3-3-2-3-基于RDD方式3"><a href="#3-3-2-3-基于RDD方式3" class="headerlink" title="3.3.2.3 基于RDD方式3"></a>3.3.2.3 基于RDD方式3</h4><p>将RDD转换为DataFrame方式3：使用RDD的<code>toDF</code>方法转换RDD</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># StructType 类</span></span><br><span class="line"><span class="comment"># 这个类 可以定义整个DataFrame中的Schema</span></span><br><span class="line">schema = StructType().\</span><br><span class="line">add(<span class="string">&quot;id&quot;</span>, IntegerType(), nullable=<span class="literal">False</span>).\</span><br><span class="line">add(<span class="string">&quot;name&quot;</span>, StringType(), nullable=<span class="literal">True</span>).\</span><br><span class="line">add(<span class="string">&quot;score&quot;</span>, IntegerType(), nullable=<span class="literal">False</span>)</span><br><span class="line"><span class="comment"># 一个add方法 定义一个列的信息, 如果有3个列, 就写三个add</span></span><br><span class="line"><span class="comment"># add方法: 参数1: 列名称, 参数2: 列类型, 参数3: 是否允许为空</span></span><br><span class="line"><span class="comment"># 方式1: 只传列名, 类型靠推断, 是否允许为空是true</span></span><br><span class="line">df = rdd.toDF([<span class="string">&#x27;id&#x27;</span>, <span class="string">&#x27;subject&#x27;</span>, <span class="string">&#x27;score&#x27;</span>])</span><br><span class="line">df.printSchema()</span><br><span class="line">df.show()</span><br><span class="line"><span class="comment"># 方式2: 传入完整的Schema描述对象StructType</span></span><br><span class="line">df = rdd.toDF(schema)</span><br><span class="line">df.printSchema()</span><br><span class="line">df.show()</span><br></pre></td></tr></table></figure>

<p>完整代码演示：<code>03_dataframe_create3.py</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding:utf8</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> StructType, StringType, IntegerType</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># 0. 构建执行环境入口对象SparkSession</span></span><br><span class="line">    spark = SparkSession.builder. \</span><br><span class="line">        appName(<span class="string">&quot;03_dataframe_create3.py&quot;</span>). \</span><br><span class="line">        master(<span class="string">&quot;local[*]&quot;</span>). \</span><br><span class="line">        getOrCreate()</span><br><span class="line"></span><br><span class="line">    sc = spark.sparkContext</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 基于RDD转换成DataFrame</span></span><br><span class="line">    rdd = sc.textFile(<span class="string">&quot;../data/input/sql/people.txt&quot;</span>). \</span><br><span class="line">        <span class="built_in">map</span>(<span class="keyword">lambda</span> x: x.split(<span class="string">&quot;,&quot;</span>)). \</span><br><span class="line">        <span class="built_in">map</span>(<span class="keyword">lambda</span> x: (x[<span class="number">0</span>], <span class="built_in">int</span>(x[<span class="number">1</span>])))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># toDF的方式构建DataFrame</span></span><br><span class="line">    df1 = rdd.toDF([<span class="string">&quot;name&quot;</span>, <span class="string">&quot;age&quot;</span>])  <span class="comment"># 只有数据列名，没有指定数据类型，只适合和数据不敏感的情况</span></span><br><span class="line">    df1.printSchema()</span><br><span class="line">    df1.show()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># toDF的方式2 通过StructType来构建</span></span><br><span class="line">    schema = StructType().add(<span class="string">&quot;name&quot;</span>, StringType(), nullable=<span class="literal">True</span>). \</span><br><span class="line">        add(<span class="string">&quot;age&quot;</span>, IntegerType(), nullable=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    df2 = rdd.toDF(schema=schema)</span><br><span class="line">    df2.printSchema()</span><br><span class="line">    df2.show()</span><br></pre></td></tr></table></figure>

<p>输出结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">root</span><br><span class="line"> |-- name: string (nullable = true)</span><br><span class="line"> |-- age: long (nullable = true)</span><br><span class="line"></span><br><span class="line">+-------+---+</span><br><span class="line">|   name|age|</span><br><span class="line">+-------+---+</span><br><span class="line">|Michael| 29|</span><br><span class="line">|   Andy| 30|</span><br><span class="line">| Justin| 19|</span><br><span class="line">+-------+---+</span><br><span class="line"></span><br><span class="line">root</span><br><span class="line"> |-- name: string (nullable = true)</span><br><span class="line"> |-- age: integer (nullable = false)</span><br><span class="line"></span><br><span class="line">+-------+---+</span><br><span class="line">|   name|age|</span><br><span class="line">+-------+---+</span><br><span class="line">|Michael| 29|</span><br><span class="line">|   Andy| 30|</span><br><span class="line">| Justin| 19|</span><br><span class="line">+-------+---+</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="3-3-2-4-基于Pandas的DF"><a href="#3-3-2-4-基于Pandas的DF" class="headerlink" title="3.3.2.4 基于Pandas的DF"></a>3.3.2.4 基于Pandas的DF</h4><p>将Pandas的DataFrame对象，转变为<code>分布式</code>的SparkSQL DataFrame对象</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 构建Pandas的DF</span></span><br><span class="line">pdf = pd.DataFrame(&#123;</span><br><span class="line"><span class="string">&quot;id&quot;</span>: [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line"><span class="string">&quot;name&quot;</span>: [<span class="string">&quot;张大仙&quot;</span>, <span class="string">&#x27;王晓晓&#x27;</span>, <span class="string">&#x27;王大锤&#x27;</span>],</span><br><span class="line"><span class="string">&quot;age&quot;</span>: [<span class="number">11</span>, <span class="number">11</span>, <span class="number">11</span>]</span><br><span class="line">&#125;)</span><br><span class="line"><span class="comment"># 将Pandas的DF对象转换成Spark的DF</span></span><br><span class="line">df = spark.createDataFrame(pdf)</span><br></pre></td></tr></table></figure>

<p>完整代码演示：<code>04_dataframe_create4.py</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding:utf8</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> StructType, StringType, IntegerType</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># 0. 构建执行环境入口对象SparkSession</span></span><br><span class="line">    spark = SparkSession.builder. \</span><br><span class="line">        appName(<span class="string">&quot;04_dataframe_create4.py&quot;</span>). \</span><br><span class="line">        master(<span class="string">&quot;local[*]&quot;</span>). \</span><br><span class="line">        getOrCreate()</span><br><span class="line"></span><br><span class="line">    sc = spark.sparkContext</span><br><span class="line">    <span class="comment"># 基于Pandas的DataFrame构建SparkSQL的DataFrame对象</span></span><br><span class="line">    pdf = pd.DataFrame(</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">&quot;id&quot;</span>: [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">            <span class="string">&quot;name&quot;</span>: [<span class="string">&quot;靓仔&quot;</span>, <span class="string">&quot;吊毛&quot;</span>, <span class="string">&quot;如花&quot;</span>],</span><br><span class="line">            <span class="string">&quot;age&quot;</span>: [<span class="number">11</span>, <span class="number">21</span>, <span class="number">11</span>]</span><br><span class="line">        &#125;</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    df = spark.createDataFrame(pdf)</span><br><span class="line">    df.printSchema()</span><br><span class="line">    df.show()</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>输出结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">root</span><br><span class="line"> |-- id: long (nullable = true)</span><br><span class="line"> |-- name: string (nullable = true)</span><br><span class="line"> |-- age: long (nullable = true)</span><br><span class="line"></span><br><span class="line">+---+----+---+</span><br><span class="line">| id|name|age|</span><br><span class="line">+---+----+---+</span><br><span class="line">|  1|靓仔| 11|</span><br><span class="line">|  2|吊毛| 21|</span><br><span class="line">|  3|如花| 11|</span><br><span class="line">+---+----+---+</span><br></pre></td></tr></table></figure>

<h4 id="3-3-2-5-读取外部数据"><a href="#3-3-2-5-读取外部数据" class="headerlink" title="3.3.2.5 读取外部数据"></a>3.3.2.5 读取外部数据</h4><p>通过SparkSQL的统一API进行数据读取构建DataFrame</p>
<p>统一API示例代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sparksession.read.<span class="built_in">format</span>(<span class="string">&quot;text|csv|json|parquet|orc|avro|jdbc|......&quot;</span>)</span><br><span class="line">.option(<span class="string">&quot;K&quot;</span>, <span class="string">&quot;V&quot;</span>) <span class="comment"># option可选</span></span><br><span class="line">.schema(StructType | String) <span class="comment"># STRING的语法如.schema(&quot;name STRING&quot;, &quot;age INT&quot;)</span></span><br><span class="line">.load(<span class="string">&quot;被读取文件的路径, 支持本地文件系统和HDFS&quot;</span>)</span><br></pre></td></tr></table></figure>

<hr>
<p><strong>读取text数据源</strong>：</p>
<p>使用<code>format(“text”)</code>读取文本数据</p>
<p>读取到的DataFrame只会有一个列，列名默认称之为：value</p>
<p>示例代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">schema = StructType().add(<span class="string">&quot;data&quot;</span>, StringType(), nullable=<span class="literal">True</span>)</span><br><span class="line">df = spark.read.<span class="built_in">format</span>(<span class="string">&quot;text&quot;</span>)\</span><br><span class="line">.schema(schema)\</span><br><span class="line">.load(<span class="string">&quot;../data/sql/people.txt&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>完整代码演示：<code>05_dataframe_create5_text.py</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding:utf8</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> StructType, StringType, IntegerType</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># 0. 构建执行环境入口对象SparkSession</span></span><br><span class="line">    spark = SparkSession.builder. \</span><br><span class="line">        appName(<span class="string">&quot;05_dataframe_create5_text.py&quot;</span>). \</span><br><span class="line">        master(<span class="string">&quot;local[*]&quot;</span>). \</span><br><span class="line">        getOrCreate()</span><br><span class="line"></span><br><span class="line">    sc = spark.sparkContext</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 构建StructType，text数据源，读取数据的特点是，将一整行只作为一个列读取，默认列名是value 类型是String</span></span><br><span class="line">    <span class="comment"># 我们设置列名为value</span></span><br><span class="line">    schema = StructType().add(<span class="string">&quot;data&quot;</span>, StringType(), nullable=<span class="literal">True</span>)</span><br><span class="line">    df = spark.read.<span class="built_in">format</span>(<span class="string">&quot;text&quot;</span>). \</span><br><span class="line">        schema(schema=schema). \</span><br><span class="line">        load(<span class="string">&quot;../data/input/sql/people.txt&quot;</span>)</span><br><span class="line"></span><br><span class="line">    df.printSchema()</span><br><span class="line">    df.show()</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>输出结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">root</span><br><span class="line"> |-- data: string (nullable = true)</span><br><span class="line"></span><br><span class="line">+-----------+</span><br><span class="line">|       data|</span><br><span class="line">+-----------+</span><br><span class="line">|Michael, 29|</span><br><span class="line">|   Andy, 30|</span><br><span class="line">| Justin, 19|</span><br><span class="line">+-----------+</span><br></pre></td></tr></table></figure>

<hr>
<p><strong>读取json数据源</strong></p>
<p>使用<code>format(&quot;json&quot;)</code>读取json数据</p>
<p>示例代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">df = spark.read.<span class="built_in">format</span>(<span class="string">&quot;json&quot;</span>).\</span><br><span class="line">load(<span class="string">&quot;../data/sql/people.json&quot;</span>)</span><br><span class="line"><span class="comment"># JSON 类型 一般不用写.schema, json自带, json带有列名 和列类型(字符串和数字)</span></span><br><span class="line">df.printSchema()</span><br><span class="line">df.show()</span><br></pre></td></tr></table></figure>

<p>完整代码演示：<code>06_dataframe_create6_json.py</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding:utf8</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> StructType, StringType, IntegerType</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># 0. 构建执行环境入口对象SparkSession</span></span><br><span class="line">    spark = SparkSession.builder. \</span><br><span class="line">        appName(<span class="string">&quot;06_dataframe_create6_json.py&quot;</span>). \</span><br><span class="line">        master(<span class="string">&quot;local[*]&quot;</span>). \</span><br><span class="line">        getOrCreate()</span><br><span class="line"></span><br><span class="line">    sc = spark.sparkContext</span><br><span class="line"></span><br><span class="line">    <span class="comment"># JSON类型自带有Schema信息</span></span><br><span class="line">    df = spark.read.<span class="built_in">format</span>(<span class="string">&quot;json&quot;</span>).load(<span class="string">&quot;../data/input/sql/people.json&quot;</span>)</span><br><span class="line">    df.printSchema()</span><br><span class="line">    df.show()</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>输出结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">root</span><br><span class="line"> |-- age: long (nullable = true)</span><br><span class="line"> |-- name: string (nullable = true)</span><br><span class="line"></span><br><span class="line">+----+-------+</span><br><span class="line">| age|   name|</span><br><span class="line">+----+-------+</span><br><span class="line">|null|Michael|</span><br><span class="line">|  30|   Andy|</span><br><span class="line">|  19| Justin|</span><br><span class="line">+----+-------+</span><br></pre></td></tr></table></figure>

<hr>
<p>读取csv数据源</p>
<p>使用<code>format(“csv”)</code>读取csv数据</p>
<p>示例代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">df = spark.read.<span class="built_in">format</span>(<span class="string">&quot;csv&quot;</span>)\</span><br><span class="line">.option(<span class="string">&quot;sep&quot;</span>, <span class="string">&quot;;&quot;</span>)\ <span class="comment"># 列分隔符</span></span><br><span class="line">.option(<span class="string">&quot;header&quot;</span>, <span class="literal">False</span>)\ <span class="comment"># 是否有CSV标头</span></span><br><span class="line">.option(<span class="string">&quot;encoding&quot;</span>, <span class="string">&quot;utf-8&quot;</span>)\ <span class="comment"># 编码</span></span><br><span class="line">.schema(<span class="string">&quot;name STRING, age INT, job STRING&quot;</span>)\ <span class="comment"># 指定列名和类型</span></span><br><span class="line">.load(<span class="string">&quot;../data/sql/people.csv&quot;</span>) <span class="comment"># 路径</span></span><br><span class="line">df.printSchema()</span><br><span class="line">df.show()</span><br></pre></td></tr></table></figure>

<p>完整代码演示：<code>07_dataframe_create7_csv.py</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding:utf8</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> StructType, StringType, IntegerType</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># 0. 构建执行环境入口对象SparkSession</span></span><br><span class="line">    spark = SparkSession.builder. \</span><br><span class="line">        appName(<span class="string">&quot;07_dataframe_create7_csv.py&quot;</span>). \</span><br><span class="line">        master(<span class="string">&quot;local[*]&quot;</span>). \</span><br><span class="line">        getOrCreate()</span><br><span class="line"></span><br><span class="line">    sc = spark.sparkContext</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 读取csv文件</span></span><br><span class="line">    df = spark.read.<span class="built_in">format</span>(<span class="string">&quot;csv&quot;</span>). \</span><br><span class="line">        option(<span class="string">&quot;sep&quot;</span>, <span class="string">&quot;;&quot;</span>). \</span><br><span class="line">        option(<span class="string">&quot;header&quot;</span>, <span class="literal">True</span>). \</span><br><span class="line">        option(<span class="string">&quot;encoding&quot;</span>, <span class="string">&quot;utf-8&quot;</span>). \</span><br><span class="line">        schema(<span class="string">&quot;name STRING,age INT,job STRING&quot;</span>). \</span><br><span class="line">        load(<span class="string">&quot;../data/input/sql/people.csv&quot;</span>)</span><br><span class="line"></span><br><span class="line">    df.printSchema()</span><br><span class="line">    df.show()</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>输出结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">root</span><br><span class="line"> |-- name: string (nullable = true)</span><br><span class="line"> |-- age: integer (nullable = true)</span><br><span class="line"> |-- job: string (nullable = true)</span><br><span class="line"></span><br><span class="line">+-----+----+---------+</span><br><span class="line">| name| age|      job|</span><br><span class="line">+-----+----+---------+</span><br><span class="line">|Jorge|  30|Developer|</span><br><span class="line">|  Bob|  32|Developer|</span><br><span class="line">|  Ani|  11|Developer|</span><br><span class="line">| Lily|  11|  Manager|</span><br><span class="line">|  Put|  11|Developer|</span><br><span class="line">|Alice|   9|  Manager|</span><br><span class="line">|Alice|   9|  Manager|</span><br><span class="line">|Alice|   9|  Manager|</span><br><span class="line">|Alice|   9|  Manager|</span><br><span class="line">|Alice|null|  Manager|</span><br><span class="line">|Alice|   9|     null|</span><br><span class="line">+-----+----+---------+</span><br></pre></td></tr></table></figure>

<hr>
<p>读取parquet数据源</p>
<p>使用<code>format(“parquet”)</code>读取parquet数据</p>
<p>示例代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># parquet 自带schema, 直接load啥也不需要了</span></span><br><span class="line">df = spark.read.<span class="built_in">format</span>(<span class="string">&quot;parquet&quot;</span>).\</span><br><span class="line">load(<span class="string">&quot;../data/sql/users.parquet&quot;</span>)</span><br><span class="line">df.printSchema()</span><br><span class="line">df.show()</span><br></pre></td></tr></table></figure>

<p>完整代码演示：<code>08_dataframe_create8_parquet.py</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding:utf8</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> StructType, StringType, IntegerType</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># 0. 构建执行环境入口对象SparkSession</span></span><br><span class="line">    spark = SparkSession.builder. \</span><br><span class="line">        appName(<span class="string">&quot;08_dataframe_create8_parquet.py&quot;</span>). \</span><br><span class="line">        master(<span class="string">&quot;local[*]&quot;</span>). \</span><br><span class="line">        getOrCreate()</span><br><span class="line"></span><br><span class="line">    sc = spark.sparkContext</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 读取parquet文件</span></span><br><span class="line">    df = spark.read.<span class="built_in">format</span>(<span class="string">&quot;parquet&quot;</span>).load(<span class="string">&quot;../data/input/sql/users.parquet&quot;</span>)</span><br><span class="line"></span><br><span class="line">    df.printSchema()</span><br><span class="line">    df.show()</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>输出结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">root</span><br><span class="line"> |-- name: string (nullable = true)</span><br><span class="line"> |-- favorite_color: string (nullable = true)</span><br><span class="line"> |-- favorite_numbers: array (nullable = true)</span><br><span class="line"> |    |-- element: integer (containsNull = true)</span><br><span class="line"></span><br><span class="line">+------+--------------+----------------+</span><br><span class="line">|  name|favorite_color|favorite_numbers|</span><br><span class="line">+------+--------------+----------------+</span><br><span class="line">|Alyssa|          null|  [3, 9, 15, 20]|</span><br><span class="line">|   Ben|           red|              []|</span><br><span class="line">+------+--------------+----------------+</span><br></pre></td></tr></table></figure>

<p>parquet: 是Spark中常用的一种列式存储文件格式，和Hive中的ORC差不多, 他俩都是列存储格式</p>
<p>parquet对比普通的文本文件的区别:</p>
<ul>
<li><p>parquet 内置schema (列名\ 列类型\ 是否为空)</p>
</li>
<li><p>存储是以列作为存储格式</p>
</li>
<li><p>存储是序列化存储在文件中的(有压缩属性体积小)</p>
</li>
</ul>
<p>Parquet文件不能直接打开查看，如果想要查看内容可以在PyCharm中安装如下插件来查看：</p>
<p><img src="https://cdn.jsdelivr.net/gh/Sql88/BlogImg@main/img/image-20230825175402666.png" alt="image-20230825175402666"></p>
<h3 id="3-3-3-DataFrame的入门操作"><a href="#3-3-3-DataFrame的入门操作" class="headerlink" title="3.3.3 DataFrame的入门操作"></a>3.3.3 DataFrame的入门操作</h3><p>DataFrame支持两种风格进行编程，分别是：</p>
<ul>
<li><p>DSL风格</p>
</li>
<li><p>SQL风格</p>
</li>
</ul>
<p><strong>DSL</strong>语法风格：</p>
<p>DSL称之为：领域特定语言。</p>
<p>其实就是指DataFrame的特有API</p>
<p>DSL风格意思就是以调用API的方式来处理Data</p>
<p>比如：<code>df.where().limit()</code></p>
<p><strong>SQL</strong>语法风格：</p>
<p>SQL风格就是使用SQL语句处理DataFrame的数据</p>
<p>比如：<code>spark.sql(“SELECT * FROM xxx)</code></p>
<h4 id="3-3-3-1-DSL风格"><a href="#3-3-3-1-DSL风格" class="headerlink" title="3.3.3.1 DSL风格"></a>3.3.3.1 DSL风格</h4><p><strong>DSL-show方法</strong></p>
<p>功能：展示DataFrame中的数据, 默认展示20条</p>
<p>语法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">df.show(参数<span class="number">1</span>, 参数<span class="number">2</span>)</span><br><span class="line">- 参数<span class="number">1</span>: 默认是<span class="number">20</span>, 控制展示多少条</span><br><span class="line">- 参数<span class="number">2</span>: 是否截断列, 默认只输出<span class="number">20</span>个字符的长度, 过长不显示, 要显示的话 请填入 truncate = <span class="literal">True</span></span><br></pre></td></tr></table></figure>

<p>如图，某个df.show后的展示结果：<img src="https://cdn.jsdelivr.net/gh/Sql88/BlogImg@main/img/image-20230825220122185.png" alt="image-20230825220122185"></p>
<hr>
<p><strong>DSL-printSchema方法</strong></p>
<p>功能：打印输出df的schema信息</p>
<p>语法：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.printSchema()</span><br></pre></td></tr></table></figure>

<p><img src="https://cdn.jsdelivr.net/gh/Sql88/BlogImg@main/img/image-20230825220212413.png" alt="image-20230825220212413"></p>
<hr>
<p><strong>DSL-select</strong></p>
<p>功能：选择DataFrame中的指定列（通过传入参数进行指定）</p>
<p>语法：</p>
<p><img src="https://cdn.jsdelivr.net/gh/Sql88/BlogImg@main/img/image-20230825222239059.png" alt="image-20230825222239059"></p>
<p>可传递：</p>
<ul>
<li><p>可变参数的cols对象，cols对象可以是Column对象来指定列或者字符串列名来指定列</p>
</li>
<li><p>List[Column]对象或者List[str]对象， 用来选择多个列</p>
</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/Sql88/BlogImg@main/img/image-20230825220357050.png" alt="image-20230825220357050"></p>
<hr>
<p><strong>DLS-filter和where</strong></p>
<p>功能：过滤DataFrame内的数据，返回一个过滤后的DataFrame</p>
<p>语法：</p>
<p><code>df.filter()</code></p>
<p><code>df.where()</code></p>
<p>where和filter功能上是等价的</p>
<p><img src="https://cdn.jsdelivr.net/gh/Sql88/BlogImg@main/img/image-20230825222215745.png" alt="image-20230825222215745"></p>
<p><img src="https://cdn.jsdelivr.net/gh/Sql88/BlogImg@main/img/image-20230825221901984.png" alt="image-20230825221901984"></p>
<hr>
<p><strong>DSL-groupBy分组</strong></p>
<p>功能：按照指定的列进行数据的分组， 返回值是GroupedData对象</p>
<p>语法：</p>
<p><code>df.groupBy()</code></p>
<p><img src="https://cdn.jsdelivr.net/gh/Sql88/BlogImg@main/img/image-20230825221842036.png" alt="image-20230825221842036"></p>
<p>传入参数和select一样，支持多种形式，不管怎么传意思就是告诉spark按照哪个列分组</p>
<p><img src="https://cdn.jsdelivr.net/gh/Sql88/BlogImg@main/img/image-20230825221832645.png" alt="image-20230825221832645"></p>
<hr>
<p><strong>GroupedData对象</strong></p>
<p>GroupedData对象是一个特殊的DataFrame数据集，其类全名：<code>&lt;class &#39;pyspark.sql.group.GroupedData&#39;&gt;</code>，这个对象是经过<code>groupBy</code>后得到的返回值， 内部记录了 以分组形式存储的数据，GroupedData对象其实也有很多API，比如前面的count方法就是这个对象的内置方法。除此之外，像：min、max、avg、sum、等等许多方法都存在，后续会再次使用它。</p>
<p>代码演示：<code>09_dataframe_process_dsl.py</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding:utf8</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> StructType, StringType, IntegerType</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># 0. 构建执行环境入口对象SparkSession</span></span><br><span class="line">    spark = SparkSession.builder. \</span><br><span class="line">        appName(<span class="string">&quot;09_dataframe_process_dsl.py&quot;</span>). \</span><br><span class="line">        master(<span class="string">&quot;local[*]&quot;</span>). \</span><br><span class="line">        getOrCreate()</span><br><span class="line"></span><br><span class="line">    sc = spark.sparkContext</span><br><span class="line"></span><br><span class="line">    df = spark.read.<span class="built_in">format</span>(<span class="string">&quot;csv&quot;</span>).\</span><br><span class="line">        schema(<span class="string">&quot;id INT,subject STRING,score INT&quot;</span>).\</span><br><span class="line">        load(<span class="string">&quot;../data/input/sql/stu_score.txt&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Column对象的获取</span></span><br><span class="line">    id_column = df[<span class="string">&#x27;id&#x27;</span>]</span><br><span class="line">    subject_column = df[<span class="string">&#x27;subject&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># DSL风格演示</span></span><br><span class="line">    df.select([<span class="string">&quot;id&quot;</span>,<span class="string">&#x27;subject&#x27;</span>]).show()</span><br><span class="line">    <span class="comment"># df.select(&quot;id&quot;,&quot;subject&quot;).show()</span></span><br><span class="line">    <span class="comment"># df.select(id_column,subject_column).show()</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># filter API</span></span><br><span class="line">    df.<span class="built_in">filter</span>(<span class="string">&quot;score &lt; 99&quot;</span>).show()</span><br><span class="line">    <span class="comment"># df.filter(df[&quot;score&quot;] &lt; 99).show()</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># # where API</span></span><br><span class="line">    df.where(<span class="string">&quot;score &lt; 99&quot;</span>).show()</span><br><span class="line">    <span class="comment"># df.where(df[&#x27;score&#x27;] &lt; 99).show()</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># group By API</span></span><br><span class="line">    df.groupBy(<span class="string">&quot;subject&quot;</span>).count().show()</span><br><span class="line">    <span class="comment"># df.groupBy(df[&quot;subject&quot;]).count().show()</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># df.groupBy API的返回值 GroupedData</span></span><br><span class="line">    <span class="comment"># GroupedData对象 不是DataFrame</span></span><br><span class="line">    <span class="comment"># 它是一个 有分组关系的数据结构，有一些API供我们对分组做聚合</span></span><br><span class="line">    <span class="comment"># SQL: group by 后接上聚合：sum avg count min max</span></span><br><span class="line">    <span class="comment"># GroupedData 类似于SQL分组后的数据结构，同样有上述5种聚合方法</span></span><br><span class="line">    <span class="comment"># GroupedData 调用聚合方法后，返回值依旧是DataFrame</span></span><br><span class="line">    <span class="comment"># GroupedData 只是一个中转的对象，最终还是要获得DataFrame的结果</span></span><br><span class="line">    r = df.groupBy(<span class="string">&quot;subject&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">type</span>(r))</span><br><span class="line">    <span class="built_in">print</span>(r.<span class="built_in">max</span>().show())</span><br><span class="line">    <span class="built_in">print</span>(r.<span class="built_in">min</span>().show())</span><br></pre></td></tr></table></figure>

<p>输出结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br></pre></td><td class="code"><pre><span class="line">+---+-------+</span><br><span class="line">| id|subject|</span><br><span class="line">+---+-------+</span><br><span class="line">|  1|   语文|</span><br><span class="line">|  2|   语文|</span><br><span class="line">|  3|   语文|</span><br><span class="line">|  4|   语文|</span><br><span class="line">|  5|   语文|</span><br><span class="line">|  6|   语文|</span><br><span class="line">|  7|   语文|</span><br><span class="line">|  8|   语文|</span><br><span class="line">|  9|   语文|</span><br><span class="line">| 10|   语文|</span><br><span class="line">| 11|   语文|</span><br><span class="line">| 12|   语文|</span><br><span class="line">| 13|   语文|</span><br><span class="line">| 14|   语文|</span><br><span class="line">| 15|   语文|</span><br><span class="line">| 16|   语文|</span><br><span class="line">| 17|   语文|</span><br><span class="line">| 18|   语文|</span><br><span class="line">| 19|   语文|</span><br><span class="line">| 20|   语文|</span><br><span class="line">+---+-------+</span><br><span class="line">only showing top 20 rows</span><br><span class="line"></span><br><span class="line">+---+-------+-----+</span><br><span class="line">| id|subject|score|</span><br><span class="line">+---+-------+-----+</span><br><span class="line">|  1|   数学|   96|</span><br><span class="line">|  2|   数学|   96|</span><br><span class="line">|  3|   数学|   96|</span><br><span class="line">|  4|   数学|   96|</span><br><span class="line">|  5|   数学|   96|</span><br><span class="line">|  6|   数学|   96|</span><br><span class="line">|  7|   数学|   96|</span><br><span class="line">|  8|   数学|   96|</span><br><span class="line">|  9|   数学|   96|</span><br><span class="line">| 10|   数学|   96|</span><br><span class="line">| 11|   数学|   96|</span><br><span class="line">| 12|   数学|   96|</span><br><span class="line">| 13|   数学|   96|</span><br><span class="line">| 14|   数学|   96|</span><br><span class="line">| 15|   数学|   96|</span><br><span class="line">| 16|   数学|   96|</span><br><span class="line">| 17|   数学|   96|</span><br><span class="line">| 18|   数学|   96|</span><br><span class="line">| 19|   数学|   96|</span><br><span class="line">| 20|   数学|   96|</span><br><span class="line">+---+-------+-----+</span><br><span class="line">only showing top 20 rows</span><br><span class="line"></span><br><span class="line">+---+-------+-----+</span><br><span class="line">| id|subject|score|</span><br><span class="line">+---+-------+-----+</span><br><span class="line">|  1|   数学|   96|</span><br><span class="line">|  2|   数学|   96|</span><br><span class="line">|  3|   数学|   96|</span><br><span class="line">|  4|   数学|   96|</span><br><span class="line">|  5|   数学|   96|</span><br><span class="line">|  6|   数学|   96|</span><br><span class="line">|  7|   数学|   96|</span><br><span class="line">|  8|   数学|   96|</span><br><span class="line">|  9|   数学|   96|</span><br><span class="line">| 10|   数学|   96|</span><br><span class="line">| 11|   数学|   96|</span><br><span class="line">| 12|   数学|   96|</span><br><span class="line">| 13|   数学|   96|</span><br><span class="line">| 14|   数学|   96|</span><br><span class="line">| 15|   数学|   96|</span><br><span class="line">| 16|   数学|   96|</span><br><span class="line">| 17|   数学|   96|</span><br><span class="line">| 18|   数学|   96|</span><br><span class="line">| 19|   数学|   96|</span><br><span class="line">| 20|   数学|   96|</span><br><span class="line">+---+-------+-----+</span><br><span class="line">only showing top 20 rows</span><br><span class="line"></span><br><span class="line">+-------+-----+</span><br><span class="line">|subject|count|</span><br><span class="line">+-------+-----+</span><br><span class="line">|   英语|   30|</span><br><span class="line">|   语文|   30|</span><br><span class="line">|   数学|   30|</span><br><span class="line">+-------+-----+</span><br><span class="line"></span><br><span class="line">&lt;class &#x27;pyspark.sql.group.GroupedData&#x27;&gt;</span><br><span class="line">+-------+-------+----------+</span><br><span class="line">|subject|max(id)|max(score)|</span><br><span class="line">+-------+-------+----------+</span><br><span class="line">|   英语|     33|        99|</span><br><span class="line">|   语文|     30|        99|</span><br><span class="line">|   数学|     30|        96|</span><br><span class="line">+-------+-------+----------+</span><br><span class="line"></span><br><span class="line">None</span><br><span class="line">+-------+-------+----------+</span><br><span class="line">|subject|min(id)|min(score)|</span><br><span class="line">+-------+-------+----------+</span><br><span class="line">|   英语|      1|        99|</span><br><span class="line">|   语文|      1|        99|</span><br><span class="line">|   数学|      1|        96|</span><br><span class="line">+-------+-------+----------+</span><br><span class="line"></span><br><span class="line">None</span><br></pre></td></tr></table></figure>



<h4 id="3-3-3-2-SQL风格"><a href="#3-3-3-2-SQL风格" class="headerlink" title="3.3.3.2 SQL风格"></a>3.3.3.2 SQL风格</h4><p><strong>SQL风格语法-注册DataFrame成为表</strong>：</p>
<p>DataFrame的一个强大之处就是我们可以将它看作是一个关系型数据表，然后可以通过在程序中，使用spark.sql() 来执行SQL语句查询，结果返回一个DataFrame。如果想使用SQL风格的语法，需要将DataFrame注册成表,采用如下的方式：</p>
<p><img src="https://cdn.jsdelivr.net/gh/Sql88/BlogImg@main/img/image-20230825221811807.png" alt="image-20230825221811807"></p>
<p><img src="https://cdn.jsdelivr.net/gh/Sql88/BlogImg@main/img/image-20230825221047238.png" alt="image-20230825221047238"></p>
<hr>
<p><strong>SQL风格语法-使用SQL查询</strong>：</p>
<p>注册好表后，可以通过：</p>
<p><code>sparksession.sql(sql语句)</code>来执行sql查询</p>
<p>返回值是一个新的df</p>
<p><strong>示例：</strong></p>
<hr>
<p><strong>pyspark.sql.functions包</strong>：</p>
<p>PySpark提供了一个包: <code>pyspark.sql.functions</code></p>
<p>这个包里面提供了 一系列的计算函数供SparkSQL使用，如何用呢？</p>
<p>导包</p>
<p><code>from pyspark.sql import functions as F</code></p>
<p>然后就可以用F对象调用函数计算了。这些功能函数, 返回值多数都是Column对象：</p>
<p><strong>示例</strong>：</p>
<p><img src="https://cdn.jsdelivr.net/gh/Sql88/BlogImg@main/img/image-20230825223308733.png" alt="image-20230825223308733"></p>
<p>代码演示：<code>10_dataframe_process_sql.py</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding:utf8</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> StructType, StringType, IntegerType</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># 0. 构建执行环境入口对象SparkSession</span></span><br><span class="line">    spark = SparkSession.builder. \</span><br><span class="line">        appName(<span class="string">&quot;10_dataframe_process_sql.py&quot;</span>). \</span><br><span class="line">        master(<span class="string">&quot;local[*]&quot;</span>). \</span><br><span class="line">        getOrCreate()</span><br><span class="line"></span><br><span class="line">    sc = spark.sparkContext</span><br><span class="line"></span><br><span class="line">    df = spark.read.<span class="built_in">format</span>(<span class="string">&quot;csv&quot;</span>). \</span><br><span class="line">        schema(<span class="string">&quot;id INT,subject STRING,score INT&quot;</span>). \</span><br><span class="line">        load(<span class="string">&quot;../data/input/sql/stu_score.txt&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 注册成临时表</span></span><br><span class="line">    df.createTempView(<span class="string">&quot;score&quot;</span>)  <span class="comment"># 注册临时视图（表）</span></span><br><span class="line">    df.createOrReplaceTempView(<span class="string">&quot;score_2&quot;</span>)  <span class="comment"># 注册 或者 替换 临时视图</span></span><br><span class="line">    df.createGlobalTempView(<span class="string">&quot;score_3&quot;</span>)  <span class="comment"># 注册全局临时视图 全局临时视图在使用的时候 需要在前面带上global_temp.前缀</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 可以通过SparkSession对象的sql api来完成sql语句的执行</span></span><br><span class="line">    spark.sql(<span class="string">&quot;select subject,count(*) as cnt from score group by subject&quot;</span>).show()</span><br><span class="line">    spark.sql(<span class="string">&quot;select subject,count(*) as cnt from score_2 group by subject&quot;</span>).show()</span><br><span class="line">    spark.sql(<span class="string">&quot;select subject,count(*) as cnt from global_temp.score_3 group by subject&quot;</span>).show()</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>输出结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">+-------+---+</span><br><span class="line">|subject|cnt|</span><br><span class="line">+-------+---+</span><br><span class="line">|   英语| 30|</span><br><span class="line">|   语文| 30|</span><br><span class="line">|   数学| 30|</span><br><span class="line">+-------+---+</span><br><span class="line"></span><br><span class="line">+-------+---+</span><br><span class="line">|subject|cnt|</span><br><span class="line">+-------+---+</span><br><span class="line">|   英语| 30|</span><br><span class="line">|   语文| 30|</span><br><span class="line">|   数学| 30|</span><br><span class="line">+-------+---+</span><br><span class="line"></span><br><span class="line">+-------+---+</span><br><span class="line">|subject|cnt|</span><br><span class="line">+-------+---+</span><br><span class="line">|   英语| 30|</span><br><span class="line">|   语文| 30|</span><br><span class="line">|   数学| 30|</span><br><span class="line">+-------+---+</span><br></pre></td></tr></table></figure>



<hr>
<h3 id="3-3-4-词频统计案例"><a href="#3-3-4-词频统计案例" class="headerlink" title="3.3.4 词频统计案例"></a>3.3.4 词频统计案例</h3><p>我们来完成一个单词计数需求，使用DSL和SQL两种风格来实现。</p>
<p>代码演示：<code>11_wordcount_demo.py</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding:utf8</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> functions <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># 0. 构建执行环境入口对象SparkSession</span></span><br><span class="line">    spark = SparkSession.builder. \</span><br><span class="line">        appName(<span class="string">&quot;11_wordcount_demo.py&quot;</span>). \</span><br><span class="line">        master(<span class="string">&quot;local[*]&quot;</span>). \</span><br><span class="line">        getOrCreate()</span><br><span class="line">    sc = spark.sparkContext</span><br><span class="line"></span><br><span class="line">    <span class="comment"># TODO 1:SQL 风格进行处理</span></span><br><span class="line">    rdd = sc.textFile(<span class="string">&quot;../data/input/words.txt&quot;</span>). \</span><br><span class="line">        flatMap(<span class="keyword">lambda</span> x: x.split(<span class="string">&quot; &quot;</span>)). \</span><br><span class="line">        <span class="built_in">map</span>(<span class="keyword">lambda</span> x: [x])</span><br><span class="line">    <span class="comment"># dataFrame需要嵌套形式的数据， 比如：[[&#x27;hello&#x27;], [&#x27;spark&#x27;], [&#x27;hello&#x27;], [&#x27;hadoop&#x27;], [&#x27;hello&#x27;], [&#x27;flink&#x27;]]</span></span><br><span class="line"></span><br><span class="line">    df = rdd.toDF([<span class="string">&quot;word&quot;</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 注册DF为表格</span></span><br><span class="line">    df.createTempView(<span class="string">&quot;words&quot;</span>)</span><br><span class="line"></span><br><span class="line">    spark.sql(<span class="string">&quot;select word,count(*) as cnt from words group by word order by cnt desc &quot;</span>).show()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># TODO 2: DSL风格处理</span></span><br><span class="line">    df = spark.read.<span class="built_in">format</span>(<span class="string">&quot;text&quot;</span>).load(<span class="string">&quot;../data/input/words.txt&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># withColumn方法</span></span><br><span class="line">    <span class="comment"># 方法功能：对已存在的列进行操作，返回一个新的列，如果名字和老列相同，那么替换，否则作为新列存在</span></span><br><span class="line">    df2 = df.withColumn(<span class="string">&quot;value&quot;</span>, F.explode(F.split(df[<span class="string">&#x27;value&#x27;</span>], <span class="string">&quot; &quot;</span>)))</span><br><span class="line">    df2.groupBy(<span class="string">&quot;value&quot;</span>). \</span><br><span class="line">        count(). \</span><br><span class="line">        withColumnRenamed(<span class="string">&quot;value&quot;</span>, <span class="string">&quot;word&quot;</span>). \</span><br><span class="line">        withColumnRenamed(<span class="string">&quot;count&quot;</span>, <span class="string">&quot;cnt&quot;</span>). \</span><br><span class="line">        orderBy(<span class="string">&quot;cnt&quot;</span>, ascending=<span class="literal">False</span>). \</span><br><span class="line">        show()</span><br></pre></td></tr></table></figure>

<p>输出结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">+------+---+</span><br><span class="line">|  word|cnt|</span><br><span class="line">+------+---+</span><br><span class="line">| hello|  3|</span><br><span class="line">| spark|  1|</span><br><span class="line">| flink|  1|</span><br><span class="line">|hadoop|  1|</span><br><span class="line">+------+---+</span><br><span class="line"></span><br><span class="line">+------+---+</span><br><span class="line">|  word|cnt|</span><br><span class="line">+------+---+</span><br><span class="line">| hello|  3|</span><br><span class="line">| spark|  1|</span><br><span class="line">|hadoop|  1|</span><br><span class="line">| flink|  1|</span><br><span class="line">+------+---+</span><br></pre></td></tr></table></figure>



<h3 id="3-5-电影数据分析"><a href="#3-5-电影数据分析" class="headerlink" title="3.5 电影数据分析"></a>3.5 电影数据分析</h3><p><strong>需求：</strong></p>
<ol>
<li>查询用户平均分</li>
<li>查询电影平均分</li>
<li>查询大于平均分的电影的数量</li>
<li>查询高分电影中（&gt;3）打分次数最多的用户，并求出此人打的平均分</li>
<li>查询每个用户文档平均打分，最低打分，最高打分</li>
<li>查询被评分超过100次的电影，的平均分排名Top100</li>
</ol>
<p>代码演示：<code>12_movie_demo.py</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding:utf8</span></span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> functions <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> StructType, StringType, IntegerType</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># 0. 构建执行环境入口对象SparkSession</span></span><br><span class="line">    spark = SparkSession.builder. \</span><br><span class="line">        appName(<span class="string">&quot;12_movie_demo.py&quot;</span>). \</span><br><span class="line">        master(<span class="string">&quot;local[*]&quot;</span>). \</span><br><span class="line">        config(<span class="string">&quot;spark.sql.shuffle.partitions&quot;</span>, <span class="number">2</span>). \</span><br><span class="line">        getOrCreate()</span><br><span class="line">    sc = spark.sparkContext</span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        spark.sql.shuffle.partitions 参数指的是，在sql计算中，shuffle算子阶段默认的分区数是200个，</span></span><br><span class="line"><span class="string">        对于集群模式来说，200个默认也算比较合适</span></span><br><span class="line"><span class="string">        如果在local下运行，200个很多，在调度上会带来额外损耗</span></span><br><span class="line"><span class="string">        所以在local下建议修改比较低 比如2/4/10均可</span></span><br><span class="line"><span class="string">        这个参数和Spark RDD中设置并行度的参数 是相互独立的</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 1. 读取数据集</span></span><br><span class="line">    schema = StructType().add(<span class="string">&quot;user_id&quot;</span>, StringType(), nullable=<span class="literal">True</span>). \</span><br><span class="line">        add(<span class="string">&quot;movie_id&quot;</span>, IntegerType(), nullable=<span class="literal">True</span>). \</span><br><span class="line">        add(<span class="string">&quot;rank&quot;</span>, IntegerType(), nullable=<span class="literal">True</span>). \</span><br><span class="line">        add(<span class="string">&quot;ts&quot;</span>, StringType(), nullable=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    df = spark.read.<span class="built_in">format</span>(<span class="string">&quot;csv&quot;</span>). \</span><br><span class="line">        option(<span class="string">&quot;sep&quot;</span>, <span class="string">&quot;\t&quot;</span>). \</span><br><span class="line">        option(<span class="string">&quot;header&quot;</span>, <span class="literal">False</span>). \</span><br><span class="line">        option(<span class="string">&quot;encoding&quot;</span>, <span class="string">&quot;utf-8&quot;</span>). \</span><br><span class="line">        schema(schema=schema). \</span><br><span class="line">        load(<span class="string">&quot;../data/input/sql/u.data&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># df.show()</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># TODO 1:用户平均分的计算</span></span><br><span class="line">    <span class="comment"># DSL实现</span></span><br><span class="line">    df.groupBy(<span class="string">&quot;user_id&quot;</span>). \</span><br><span class="line">        avg(<span class="string">&quot;rank&quot;</span>). \</span><br><span class="line">        withColumnRenamed(<span class="string">&quot;avg(rank)&quot;</span>, <span class="string">&quot;avg_rank&quot;</span>). \</span><br><span class="line">        withColumn(<span class="string">&quot;avg_rank&quot;</span>, F.<span class="built_in">round</span>(<span class="string">&quot;avg_rank&quot;</span>, <span class="number">2</span>)). \</span><br><span class="line">        orderBy(<span class="string">&quot;avg_rank&quot;</span>, ascending=<span class="literal">False</span>). \</span><br><span class="line">        show()</span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># # SQL实现</span></span><br><span class="line">    <span class="comment"># df.createTempView(&quot;movie&quot;)</span></span><br><span class="line">    <span class="comment"># spark.sql(&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment">#     select user_id,round(avg(rank),2) as avg_rank from movie group by user_id order by avg_rank desc</span></span><br><span class="line">    <span class="comment"># &quot;&quot;&quot;).show()</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;*&quot;</span> * <span class="number">200</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># TODO 2: 电影的平均分查询</span></span><br><span class="line">    df.createTempView(<span class="string">&quot;movie&quot;</span>)</span><br><span class="line">    spark.sql(<span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        select movie_id,round(avg(rank),2) as avg_rank from movie group by movie_id order by avg_rank desc</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span>).show()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;*&quot;</span> * <span class="number">200</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># TODO 3: 查询大于平均分的电影的数量</span></span><br><span class="line">    <span class="comment"># DSL实现</span></span><br><span class="line">    above_average_scores = df.where(df[<span class="string">&#x27;rank&#x27;</span>] &gt; df.select(F.avg(df[<span class="string">&#x27;rank&#x27;</span>])).first()[<span class="string">&#x27;avg(rank)&#x27;</span>]).count()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;大于平均分电影数量数：&quot;</span>, above_average_scores)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># # SQL实现</span></span><br><span class="line">    <span class="comment"># df.createTempView(&quot;movie&quot;)</span></span><br><span class="line">    <span class="comment"># spark.sql(&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment">#     select count(user_id) as above_average_scores from movie</span></span><br><span class="line">    <span class="comment">#     where rank &gt;(</span></span><br><span class="line">    <span class="comment">#         select avg(rank) as avg_rank from movie</span></span><br><span class="line">    <span class="comment">#     )</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># &quot;&quot;&quot;).show()</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;*&quot;</span> * <span class="number">200</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># TODO 4: 查询高分电影中（&gt;3）打分次数最多的用户，此人打分的平均分</span></span><br><span class="line">    <span class="comment"># DSL实现</span></span><br><span class="line">    <span class="comment"># 先找出这个人</span></span><br><span class="line">    user_id = df.where(<span class="string">&quot;rank&gt;3&quot;</span>). \</span><br><span class="line">        groupBy(<span class="string">&quot;user_id&quot;</span>). \</span><br><span class="line">        count(). \</span><br><span class="line">        withColumnRenamed(<span class="string">&quot;count&quot;</span>, <span class="string">&quot;cnt&quot;</span>). \</span><br><span class="line">        orderBy(<span class="string">&quot;cnt&quot;</span>, ascending=<span class="literal">False</span>). \</span><br><span class="line">        first()[<span class="string">&quot;user_id&quot;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(user_id)</span><br><span class="line">    <span class="comment"># 计算这个人的打分平均分</span></span><br><span class="line">    df.<span class="built_in">filter</span>(df[<span class="string">&#x27;user_id&#x27;</span>] == user_id). \</span><br><span class="line">        select(F.<span class="built_in">round</span>(F.avg(<span class="string">&quot;rank&quot;</span>), <span class="number">2</span>)).show()</span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># # SQL实现</span></span><br><span class="line">    <span class="comment"># df.createTempView(&quot;movie&quot;)</span></span><br><span class="line">    <span class="comment"># spark.sql(&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment">#     select round(avg(rank),2) from movie where user_id = (</span></span><br><span class="line">    <span class="comment">#         select user_id from(</span></span><br><span class="line">    <span class="comment">#             select user_id,count(user_id) as cnt from movie  where rank &gt; 3 group by user_id order by cnt desc limit 1</span></span><br><span class="line">    <span class="comment">#         )</span></span><br><span class="line">    <span class="comment">#     )</span></span><br><span class="line">    <span class="comment"># &quot;&quot;&quot;).show()</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;*&quot;</span> * <span class="number">200</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># TODO 5:查询每个用户的平均打分，最低打分，最高打分</span></span><br><span class="line">    <span class="comment"># DSL实现</span></span><br><span class="line">    df.groupBy(<span class="string">&quot;user_id&quot;</span>). \</span><br><span class="line">        agg(</span><br><span class="line">        F.<span class="built_in">round</span>(F.avg(<span class="string">&quot;rank&quot;</span>), <span class="number">2</span>).alias(<span class="string">&quot;avg_rank&quot;</span>),</span><br><span class="line">        F.<span class="built_in">min</span>(<span class="string">&quot;rank&quot;</span>).alias(<span class="string">&quot;min_rank&quot;</span>),</span><br><span class="line">        F.<span class="built_in">max</span>(<span class="string">&quot;rank&quot;</span>).alias(<span class="string">&quot;max_rank&quot;</span>)</span><br><span class="line">    ).show()</span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># # SQL实现</span></span><br><span class="line">    <span class="comment"># df.createTempView(&quot;movie&quot;)</span></span><br><span class="line">    <span class="comment"># spark.sql(&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment">#     select user_id,round(avg(rank),2) as avg_rank,min(rank) as min_rank,max(rank) as max_rank from movie</span></span><br><span class="line">    <span class="comment">#     group by user_id  order by user_id</span></span><br><span class="line">    <span class="comment"># &quot;&quot;&quot;).show()  # 默认只显示20条</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;*&quot;</span> * <span class="number">200</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># TODO 6: 查询评分超过100次的电影，的平均分 排名Top10</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># DSL实现</span></span><br><span class="line">    df.groupBy(<span class="string">&quot;movie_id&quot;</span>). \</span><br><span class="line">        agg(</span><br><span class="line">        F.count(<span class="string">&quot;movie_id&quot;</span>).alias(<span class="string">&quot;cnt&quot;</span>),</span><br><span class="line">        F.<span class="built_in">round</span>(F.avg(<span class="string">&quot;rank&quot;</span>), <span class="number">2</span>).alias(<span class="string">&quot;avg_rank&quot;</span>)</span><br><span class="line">    ).where(<span class="string">&quot;cnt &gt; 100&quot;</span>). \</span><br><span class="line">        orderBy(<span class="string">&quot;avg_rank&quot;</span>, ascending=<span class="literal">False</span>). \</span><br><span class="line">        limit(<span class="number">10</span>). \</span><br><span class="line">        show()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># # SQL实现</span></span><br><span class="line">    <span class="comment"># df.createTempView(&quot;movie&quot;)</span></span><br><span class="line">    <span class="comment"># spark.sql(&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment">#     select movie_id,m_cnt,avg_rank from</span></span><br><span class="line">    <span class="comment">#         (select movie_id,count(movie_id) as m_cnt,round(avg(rank),2) as avg_rank from movie group by movie_id)</span></span><br><span class="line">    <span class="comment">#         where m_cnt &gt; 100 order by avg_rank desc limit 10</span></span><br><span class="line">    <span class="comment"># &quot;&quot;&quot;).show()</span></span><br><span class="line"></span><br><span class="line">    time.sleep(<span class="number">100000</span>)</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">1. agg: 它是GroupData对象的API，作用是 在里面可以写多个聚合</span></span><br><span class="line"><span class="string">2. alias: 它是Column对象的API，可以针对一个列 进行改名</span></span><br><span class="line"><span class="string">3. withColumnRenamed：它是DataFrame的API，可以对DF中的列进行改名，一次改一个列，改多个列 可以链式调用</span></span><br><span class="line"><span class="string">4. orderBy：DataFrame的API，进行排序，参数1是被排序的列，参数2是 升序（True） 或降序 False</span></span><br><span class="line"><span class="string">5. first：DataFrame的API，取出DF的第一行数据，返回结果是Row对象</span></span><br><span class="line"><span class="string"># Row对象 就是一个数组，你可以通过row[&#x27;列名&#x27;]来取出当前行中，某一列的具体数值，</span></span><br><span class="line"><span class="string"># 返回值不再是DF或者GroupData或者Column，而是具体的字符串或者数字    </span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>输出结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br></pre></td><td class="code"><pre><span class="line">+-------+--------+</span><br><span class="line">|user_id|avg_rank|</span><br><span class="line">+-------+--------+</span><br><span class="line">|    849|    4.87|</span><br><span class="line">|    688|    4.83|</span><br><span class="line">|    507|    4.72|</span><br><span class="line">|    628|     4.7|</span><br><span class="line">|    928|    4.69|</span><br><span class="line">|    118|    4.66|</span><br><span class="line">|    907|    4.57|</span><br><span class="line">|    686|    4.56|</span><br><span class="line">|    427|    4.55|</span><br><span class="line">|    565|    4.54|</span><br><span class="line">|    469|    4.53|</span><br><span class="line">|    850|    4.53|</span><br><span class="line">|    225|    4.52|</span><br><span class="line">|    330|     4.5|</span><br><span class="line">|    477|    4.46|</span><br><span class="line">|    242|    4.45|</span><br><span class="line">|    636|    4.45|</span><br><span class="line">|    583|    4.44|</span><br><span class="line">|    252|    4.43|</span><br><span class="line">|    767|    4.43|</span><br><span class="line">+-------+--------+</span><br><span class="line">only showing top 20 rows</span><br><span class="line"></span><br><span class="line">********************************************************************************************************************************************************************************************************</span><br><span class="line">+--------+--------+</span><br><span class="line">|movie_id|avg_rank|</span><br><span class="line">+--------+--------+</span><br><span class="line">|    1500|     5.0|</span><br><span class="line">|    1201|     5.0|</span><br><span class="line">|    1189|     5.0|</span><br><span class="line">|    1536|     5.0|</span><br><span class="line">|    1293|     5.0|</span><br><span class="line">|    1653|     5.0|</span><br><span class="line">|    1599|     5.0|</span><br><span class="line">|    1467|     5.0|</span><br><span class="line">|    1122|     5.0|</span><br><span class="line">|     814|     5.0|</span><br><span class="line">|    1449|    4.63|</span><br><span class="line">|     119|     4.5|</span><br><span class="line">|    1398|     4.5|</span><br><span class="line">|    1594|     4.5|</span><br><span class="line">|    1642|     4.5|</span><br><span class="line">|     408|    4.49|</span><br><span class="line">|     318|    4.47|</span><br><span class="line">|     169|    4.47|</span><br><span class="line">|     483|    4.46|</span><br><span class="line">|      64|    4.45|</span><br><span class="line">+--------+--------+</span><br><span class="line">only showing top 20 rows</span><br><span class="line"></span><br><span class="line">********************************************************************************************************************************************************************************************************</span><br><span class="line">大于平均分电影数量数： 55375</span><br><span class="line">********************************************************************************************************************************************************************************************************</span><br><span class="line">450</span><br><span class="line">+-------------------+</span><br><span class="line">|round(avg(rank), 2)|</span><br><span class="line">+-------------------+</span><br><span class="line">|               3.86|</span><br><span class="line">+-------------------+</span><br><span class="line"></span><br><span class="line">********************************************************************************************************************************************************************************************************</span><br><span class="line">+-------+--------+--------+--------+</span><br><span class="line">|user_id|avg_rank|min_rank|max_rank|</span><br><span class="line">+-------+--------+--------+--------+</span><br><span class="line">|    186|    3.41|       1|       5|</span><br><span class="line">|    244|    3.65|       1|       5|</span><br><span class="line">|    200|    4.03|       2|       5|</span><br><span class="line">|    210|    4.06|       2|       5|</span><br><span class="line">|    303|    3.37|       1|       5|</span><br><span class="line">|    122|    3.98|       1|       5|</span><br><span class="line">|    194|    2.96|       1|       5|</span><br><span class="line">|    291|    3.69|       1|       5|</span><br><span class="line">|    119|    3.95|       1|       5|</span><br><span class="line">|    167|    3.38|       1|       5|</span><br><span class="line">|    299|    3.46|       1|       5|</span><br><span class="line">|    102|    2.62|       1|       4|</span><br><span class="line">|    160|    3.92|       1|       5|</span><br><span class="line">|    225|    4.52|       2|       5|</span><br><span class="line">|    290|    3.35|       1|       5|</span><br><span class="line">|     97|    4.16|       1|       5|</span><br><span class="line">|    157|    3.78|       1|       5|</span><br><span class="line">|    201|    3.03|       1|       5|</span><br><span class="line">|    287|    4.11|       1|       5|</span><br><span class="line">|    246|    2.93|       1|       5|</span><br><span class="line">+-------+--------+--------+--------+</span><br><span class="line">only showing top 20 rows</span><br><span class="line"></span><br><span class="line">********************************************************************************************************************************************************************************************************</span><br><span class="line">+--------+---+--------+</span><br><span class="line">|movie_id|cnt|avg_rank|</span><br><span class="line">+--------+---+--------+</span><br><span class="line">|     408|112|    4.49|</span><br><span class="line">|     169|118|    4.47|</span><br><span class="line">|     318|298|    4.47|</span><br><span class="line">|     483|243|    4.46|</span><br><span class="line">|      64|283|    4.45|</span><br><span class="line">|     603|209|    4.39|</span><br><span class="line">|      12|267|    4.39|</span><br><span class="line">|      50|583|    4.36|</span><br><span class="line">|     178|125|    4.34|</span><br><span class="line">|     427|219|    4.29|</span><br><span class="line">+--------+---+--------+</span><br></pre></td></tr></table></figure>



<h3 id="3-3-6-SparkSQL-Shuffle-分区数目"><a href="#3-3-6-SparkSQL-Shuffle-分区数目" class="headerlink" title="3.3.6 SparkSQL Shuffle 分区数目"></a>3.3.6 SparkSQL Shuffle 分区数目</h3><p>运行上述程序时，查看WEB UI监控页面发现，某个Stage中有200个Task任务，也就是说RDD有200分区Partition。<img src="https://cdn.jsdelivr.net/gh/Sql88/BlogImg@main/img/image-20230909151638832.png" alt="image-20230909151638832"></p>
<p>原因：在SparkSQL中当Job中产生Shuffle时，默认的分区数(<code>spark.sql.shuffle.partitions</code>) 为200，在实际项目中要合理的设置。</p>
<p>可以设置在：</p>
<ol>
<li><p>配置文件: conf&#x2F;spark-defaults.conf: spark.sql.shuffle.partitions 100</p>
</li>
<li><p>在客户端提交参数中：bin&#x2F;spark-submit –conf “spark.sql.shuffle.partitions &#x3D; 100”</p>
</li>
<li><p>在代码中可以设置：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">spark = SparkSession.builder. \</span><br><span class="line">    appName(<span class="string">&quot;12_movie_demo.py&quot;</span>). \</span><br><span class="line">    master(<span class="string">&quot;local[*]&quot;</span>). \</span><br><span class="line">    config(<span class="string">&quot;spark.sql.shuffle.partitions&quot;</span>, <span class="number">2</span>). \</span><br><span class="line">    getOrCreate()</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="3-3-7-SparkSQL-数据清洗API"><a href="#3-3-7-SparkSQL-数据清洗API" class="headerlink" title="3.3.7 SparkSQL 数据清洗API"></a>3.3.7 SparkSQL 数据清洗API</h3><p>前面我们处理的数据实际上都是已经被处理好的规整数据，但是在大数据整个生产过程中，需要先对数据进行数据清洗，将杂乱无章的数据整理为符合后面处理要求的规整数据。</p>
<p><strong>去重方法：dropDuplicates</strong></p>
<p>功能：对DF的数据进行去重，如果重复数据有多条，取第一条</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 去重API dropDuplicates，无参数是对数据进行整体去重</span></span><br><span class="line">df.dropDuplicates().show()</span><br><span class="line"><span class="comment"># API 同样可以针对字段进行去重，如下传入age字段，表示只要年龄一样 就认为你是重复数据</span></span><br><span class="line">df.dropDuplicates([<span class="string">&#x27;age&#x27;</span>,<span class="string">&#x27;job&#x27;</span>]).show()</span><br></pre></td></tr></table></figure>

<hr>
<p><strong>删除有缺失值的行方法 :dropna</strong></p>
<p>功能：如果数据中包含null，通过dropna来进行判断，符合条件就删除这一行数据</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 如果有缺失值，进行数据删除</span></span><br><span class="line"><span class="comment"># 无参数 为 how=any执行，只要有一个列是null 数据整行删除，如果填入how=&#x27;all&#x27;表示全部列为空 才会删除，how参数默认是any</span></span><br><span class="line">df.dropna().show()</span><br><span class="line"><span class="comment"># 指定阀值进行删除，tresh=3 表示，有效的列最少有3个，这行数据才保留</span></span><br><span class="line"><span class="comment"># 设定thresh后，how参数无效了</span></span><br><span class="line">df.dropna(thresh=<span class="number">3</span>).show()</span><br><span class="line"><span class="comment"># 可以指定阀值 以及配合指定列进行工作</span></span><br><span class="line"><span class="comment"># thresh=2，subset=[&#x27;name&#x27;,&#x27;age&#x27;]表示针对 这两个列，有效列最少为2个才保留数据</span></span><br><span class="line">df.dropna(thresh=<span class="number">2</span>,subset=[<span class="string">&#x27;name&#x27;</span>,<span class="string">&#x27;age&#x27;</span>]).show()</span><br></pre></td></tr></table></figure>



<hr>
<p><strong>填充缺失值数据 fillna</strong></p>
<p>功能：根据参数的规则，来进行null替换</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将所有的空，按照你指定的值进行填充，不理会列 任何空都被填充</span></span><br><span class="line">df.fillna(<span class="string">&quot;loss&quot;</span>).show()</span><br><span class="line"><span class="comment"># 指定列进行填充</span></span><br><span class="line">df.fillna(<span class="string">&quot;loss&quot;</span>,subset=[<span class="string">&#x27;job&#x27;</span>]).show()</span><br><span class="line"><span class="comment"># 给定字典 设定各个列的填充规则</span></span><br><span class="line">df.fillna(&#123;<span class="string">&quot;name&quot;</span>:<span class="string">&quot;未知姓名&quot;</span>,<span class="string">&quot;age&quot;</span>:<span class="number">1</span>,<span class="string">&quot;job&quot;</span>:<span class="string">&quot;worker&quot;</span>&#125;).show()</span><br></pre></td></tr></table></figure>

<p>完整代码演示：<code>13_data_clear_api.py</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding:utf8</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> functions <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># 0. 构建执行环境入口对象SparkSession</span></span><br><span class="line">    spark = SparkSession.builder. \</span><br><span class="line">        appName(<span class="string">&quot;13_data_clear_api.py&quot;</span>). \</span><br><span class="line">        master(<span class="string">&quot;local[*]&quot;</span>). \</span><br><span class="line">        getOrCreate()</span><br><span class="line">    sc = spark.sparkContext</span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;读取数据&quot;&quot;&quot;</span></span><br><span class="line">    df = spark.read.<span class="built_in">format</span>(<span class="string">&quot;csv&quot;</span>). \</span><br><span class="line">        option(<span class="string">&quot;sep&quot;</span>, <span class="string">&quot;;&quot;</span>). \</span><br><span class="line">        option(<span class="string">&quot;header&quot;</span>, <span class="literal">True</span>). \</span><br><span class="line">        load(<span class="string">&quot;../data/input/sql/people.csv&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 数据清洗1：数据去重</span></span><br><span class="line">    <span class="comment"># dropDuplicates 是DataFrame的API，可以完成数据去重</span></span><br><span class="line">    <span class="comment"># 无参数使用，对全部的列 联合起来进行比较，去除重复值，只保留一条</span></span><br><span class="line">    df.dropDuplicates().show()</span><br><span class="line">    df.dropDuplicates([<span class="string">&#x27;age&#x27;</span>, <span class="string">&#x27;job&#x27;</span>]).show()  <span class="comment"># 按指定列去重</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 数据清洗2：缺失值处理</span></span><br><span class="line">    <span class="comment"># dropna api 是可以对缺失值数据进行删除</span></span><br><span class="line">    <span class="comment"># 无参数使用，只要列中有null，就删除这一行数据</span></span><br><span class="line">    <span class="comment"># df.dropna().show()</span></span><br><span class="line">    <span class="comment"># # thresh = 3 表示，最少满足3个有效列，不满足则删除当前行数据</span></span><br><span class="line">    <span class="comment"># df.dropna(thresh=3).show()</span></span><br><span class="line">    <span class="comment"># df.dropna(thresh=2, subset=[&#x27;name&#x27;, &#x27;age&#x27;]).show()</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 缺失值处理也可以完成对缺失值进行填充</span></span><br><span class="line">    <span class="comment"># DataFrame的fillna 对缺失值的列进行填充</span></span><br><span class="line">    <span class="comment"># 全局填充</span></span><br><span class="line">    df.fillna(<span class="string">&quot;loss&quot;</span>).show()</span><br><span class="line">    <span class="comment"># # 指定列进行填充</span></span><br><span class="line">    <span class="comment"># df.fillna(&quot;N/A&quot;, subset=[&#x27;job&#x27;]).show()</span></span><br><span class="line">    <span class="comment"># # 设定一个字典，对所有的列提供填充规则</span></span><br><span class="line">    <span class="comment"># df.fillna(&#123;&quot;name&quot;: &quot;未知姓名&quot;, &quot;age&quot;: 1, &quot;job&quot;: &quot;worker&quot;&#125;).show()</span></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>输出结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">+-----+----+---------+</span><br><span class="line">| name| age|      job|</span><br><span class="line">+-----+----+---------+</span><br><span class="line">|Alice|   9|     null|</span><br><span class="line">|  Ani|  11|Developer|</span><br><span class="line">|Alice|   9|  Manager|</span><br><span class="line">|Alice|null|  Manager|</span><br><span class="line">| Lily|  11|  Manager|</span><br><span class="line">|  Put|  11|Developer|</span><br><span class="line">|Jorge|  30|Developer|</span><br><span class="line">|  Bob|  32|Developer|</span><br><span class="line">+-----+----+---------+</span><br><span class="line"></span><br><span class="line">+-----+----+---------+</span><br><span class="line">| name| age|      job|</span><br><span class="line">+-----+----+---------+</span><br><span class="line">|Alice|null|  Manager|</span><br><span class="line">|  Ani|  11|Developer|</span><br><span class="line">| Lily|  11|  Manager|</span><br><span class="line">|Jorge|  30|Developer|</span><br><span class="line">|  Bob|  32|Developer|</span><br><span class="line">|Alice|   9|     null|</span><br><span class="line">|Alice|   9|  Manager|</span><br><span class="line">+-----+----+---------+</span><br><span class="line"></span><br><span class="line">+-----+----+---------+</span><br><span class="line">| name| age|      job|</span><br><span class="line">+-----+----+---------+</span><br><span class="line">|Jorge|  30|Developer|</span><br><span class="line">|  Bob|  32|Developer|</span><br><span class="line">|  Ani|  11|Developer|</span><br><span class="line">| Lily|  11|  Manager|</span><br><span class="line">|  Put|  11|Developer|</span><br><span class="line">|Alice|   9|  Manager|</span><br><span class="line">|Alice|   9|  Manager|</span><br><span class="line">|Alice|   9|  Manager|</span><br><span class="line">|Alice|   9|  Manager|</span><br><span class="line">|Alice|loss|  Manager|</span><br><span class="line">|Alice|   9|     loss|</span><br><span class="line">+-----+----+---------+</span><br></pre></td></tr></table></figure>



<h3 id="3-3-8-DataFrame数据写出"><a href="#3-3-8-DataFrame数据写出" class="headerlink" title="3.3.8 DataFrame数据写出"></a>3.3.8 DataFrame数据写出</h3><p>统一API语法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">df.write.mode().<span class="built_in">format</span>().option(K,V).save(PATH)</span><br><span class="line"><span class="comment"># mode,传入模式字符串可选：append追加，overwrite覆盖，ignore忽略，error重复就报异常（默认的）</span></span><br><span class="line"><span class="comment"># format，传入格式字符串，可选：text,csv,json,parquet,orc,avro,jdbc</span></span><br><span class="line"><span class="comment"># 注意text源只支持单列df写出</span></span><br><span class="line"><span class="comment"># option设置属性，如：.option(&quot;sep&quot;,&quot;,&quot;)</span></span><br><span class="line"><span class="comment"># save 写出的路径，支持本地文件和HDFS</span></span><br></pre></td></tr></table></figure>

<p>完整代码演示：<code>14_dataframe_write.py</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding:utf8</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> functions <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> StructType, StringType, IntegerType</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># 0. 构建执行环境入口对象SparkSession</span></span><br><span class="line">    spark = SparkSession.builder. \</span><br><span class="line">        appName(<span class="string">&quot;14_dataframe_write.py&quot;</span>). \</span><br><span class="line">        master(<span class="string">&quot;local[*]&quot;</span>). \</span><br><span class="line">        getOrCreate()</span><br><span class="line">    sc = spark.sparkContext</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 1.读取数据集</span></span><br><span class="line">    schema = StructType().add(<span class="string">&quot;user_id&quot;</span>, StringType(), nullable=<span class="literal">True</span>). \</span><br><span class="line">        add(<span class="string">&quot;movie_id&quot;</span>, IntegerType(), nullable=<span class="literal">True</span>). \</span><br><span class="line">        add(<span class="string">&quot;rank&quot;</span>, IntegerType(), nullable=<span class="literal">True</span>). \</span><br><span class="line">        add(<span class="string">&quot;ts&quot;</span>, StringType(), nullable=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    df = spark.read.<span class="built_in">format</span>(<span class="string">&quot;csv&quot;</span>). \</span><br><span class="line">        option(<span class="string">&quot;sep&quot;</span>, <span class="string">&quot;\t&quot;</span>). \</span><br><span class="line">        option(<span class="string">&quot;header&quot;</span>, <span class="literal">False</span>). \</span><br><span class="line">        option(<span class="string">&quot;encoding&quot;</span>, <span class="string">&quot;utf-8&quot;</span>). \</span><br><span class="line">        schema(schema=schema). \</span><br><span class="line">        load(<span class="string">&quot;../data/input/sql/u.data&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Write Text写出，只能写出一个列的数据，需要将df转换为单列df</span></span><br><span class="line">    df.select(F.concat_ws(<span class="string">&quot;---&quot;</span>, <span class="string">&quot;user_id&quot;</span>, <span class="string">&quot;movie_id&quot;</span>, <span class="string">&quot;rank&quot;</span>, <span class="string">&quot;ts&quot;</span>)). \</span><br><span class="line">        write. \</span><br><span class="line">        mode(<span class="string">&quot;overwrite&quot;</span>). \</span><br><span class="line">        <span class="built_in">format</span>(<span class="string">&quot;text&quot;</span>). \</span><br><span class="line">        save(<span class="string">&quot;../data/output/sql/text&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Write csv</span></span><br><span class="line">    df.write.mode(<span class="string">&quot;overwrite&quot;</span>). \</span><br><span class="line">        <span class="built_in">format</span>(<span class="string">&quot;csv&quot;</span>). \</span><br><span class="line">        option(<span class="string">&quot;sep&quot;</span>, <span class="string">&quot;;&quot;</span>). \</span><br><span class="line">        option(<span class="string">&quot;header&quot;</span>, <span class="literal">True</span>). \</span><br><span class="line">        save(<span class="string">&quot;../data/output/sql/csv&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># write json</span></span><br><span class="line">    df.write.mode(<span class="string">&quot;overwrite&quot;</span>).\</span><br><span class="line">        <span class="built_in">format</span>(<span class="string">&quot;json&quot;</span>).\</span><br><span class="line">        save(<span class="string">&quot;../data/output/sql/json&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># write parquet</span></span><br><span class="line">    df.write.mode(<span class="string">&quot;overwrite&quot;</span>).\</span><br><span class="line">        <span class="built_in">format</span>(<span class="string">&quot;parquet&quot;</span>).\</span><br><span class="line">        save(<span class="string">&quot;../data/output/sql/parquet&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h3 id="3-3-9-DataFrame-通过JDBC读写数据库（MySQL示例）"><a href="#3-3-9-DataFrame-通过JDBC读写数据库（MySQL示例）" class="headerlink" title="3.3.9 DataFrame 通过JDBC读写数据库（MySQL示例）"></a>3.3.9 DataFrame 通过JDBC读写数据库（MySQL示例）</h3><p>读取JDBC是需要有驱动的，我们读取的是<code>jdbc:mysql://</code>这个协议，也就是读取的是mysql的数据</p>
<p>既然如此，就需要有msyql的驱动jar包给spark程序用</p>
<p>如果不给驱动jar包，会提示<code>No suitable Driver</code></p>
<p><img src="https://cdn.jsdelivr.net/gh/Sql88/BlogImg@main/img/image-20230909194609652.png" alt="image-20230909194609652"></p>
<p>对于windows系统（使用本地解释器）（以Anaconda环境演示）</p>
<p>将jar包放在：<code>Anaconda3的安装路径下\envs\虚拟环境\Lib\site-packages\pyspark\jars</code></p>
<p>对于Linux系统（使用远程解释器执行）（以Anaconda环境演示）</p>
<p>将jar包放在：<code>Anaconda3的安装路径下/envs/虚拟环境/lib/python3.8/site-packages/pyspark/jars</code></p>
<p>代码演示：<code>15_dataframe_jdbc.py</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding:utf8</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> functions <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> StructType, StringType, IntegerType</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># 0. 构建执行环境入口对象SparkSession</span></span><br><span class="line">    spark = SparkSession.builder. \</span><br><span class="line">        appName(<span class="string">&quot;15_dataframe_jdbc.py&quot;</span>). \</span><br><span class="line">        master(<span class="string">&quot;local[*]&quot;</span>). \</span><br><span class="line">        getOrCreate()</span><br><span class="line">    sc = spark.sparkContext</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 1.读取数据集</span></span><br><span class="line">    schema = StructType().add(<span class="string">&quot;user_id&quot;</span>, StringType(), nullable=<span class="literal">True</span>). \</span><br><span class="line">        add(<span class="string">&quot;movie_id&quot;</span>, IntegerType(), nullable=<span class="literal">True</span>). \</span><br><span class="line">        add(<span class="string">&quot;rank&quot;</span>, IntegerType(), nullable=<span class="literal">True</span>). \</span><br><span class="line">        add(<span class="string">&quot;ts&quot;</span>, StringType(), nullable=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    df = spark.read.<span class="built_in">format</span>(<span class="string">&quot;csv&quot;</span>). \</span><br><span class="line">        option(<span class="string">&quot;sep&quot;</span>, <span class="string">&quot;\t&quot;</span>). \</span><br><span class="line">        option(<span class="string">&quot;header&quot;</span>, <span class="literal">False</span>). \</span><br><span class="line">        option(<span class="string">&quot;encoding&quot;</span>, <span class="string">&quot;utf-8&quot;</span>). \</span><br><span class="line">        schema(schema=schema). \</span><br><span class="line">        load(<span class="string">&quot;../data/input/sql/u.data&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># # # 1. 写出df到msyql数据库中</span></span><br><span class="line">    <span class="comment"># df.write.mode(&quot;overwrite&quot;). \</span></span><br><span class="line">    <span class="comment">#     format(&quot;jdbc&quot;). \</span></span><br><span class="line">    <span class="comment">#     option(&quot;url&quot;, &quot;jdbc:mysql://Tnode1:3306/bigdata?useSSL=false&amp;useUnicode=true&quot;). \</span></span><br><span class="line">    <span class="comment">#     option(&quot;dbtable&quot;, &quot;movie_data&quot;). \</span></span><br><span class="line">    <span class="comment">#     option(&quot;user&quot;, &quot;root&quot;). \</span></span><br><span class="line">    <span class="comment">#     option(&quot;password&quot;, &quot;123456&quot;). \</span></span><br><span class="line">    <span class="comment">#     save()</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 2. 通过jdbc从mysql中读取数据</span></span><br><span class="line">    df2 = spark.read.<span class="built_in">format</span>(<span class="string">&quot;jdbc&quot;</span>). \</span><br><span class="line">        option(<span class="string">&quot;url&quot;</span>, <span class="string">&quot;jdbc:mysql://Tnode1:3306/bigdata?useSSl=false&amp;useUnicode=true&quot;</span>). \</span><br><span class="line">        option(<span class="string">&quot;dbtable&quot;</span>, <span class="string">&quot;movie_data&quot;</span>). \</span><br><span class="line">        option(<span class="string">&quot;user&quot;</span>, <span class="string">&quot;root&quot;</span>). \</span><br><span class="line">        option(<span class="string">&quot;password&quot;</span>, <span class="string">&quot;123456&quot;</span>). \</span><br><span class="line">        load()</span><br><span class="line">    df2.printSchema()</span><br><span class="line">    df2.show()</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">JDBC写出，会自动创建表的。</span></span><br><span class="line"><span class="string">因为DadaFrame中有表结构信息，StructType记录的各个字段的名称类型和是否运行为空</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>输出结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">root</span><br><span class="line"> |-- user_id: string (nullable = true)</span><br><span class="line"> |-- movie_id: integer (nullable = true)</span><br><span class="line"> |-- rank: integer (nullable = true)</span><br><span class="line"> |-- ts: string (nullable = true)</span><br><span class="line"></span><br><span class="line">+-------+--------+----+---------+</span><br><span class="line">|user_id|movie_id|rank|       ts|</span><br><span class="line">+-------+--------+----+---------+</span><br><span class="line">|    196|     242|   3|881250949|</span><br><span class="line">|    186|     302|   3|891717742|</span><br><span class="line">|     22|     377|   1|878887116|</span><br><span class="line">|    244|      51|   2|880606923|</span><br><span class="line">|    166|     346|   1|886397596|</span><br><span class="line">|    298|     474|   4|884182806|</span><br><span class="line">|    115|     265|   2|881171488|</span><br><span class="line">|    253|     465|   5|891628467|</span><br><span class="line">|    305|     451|   3|886324817|</span><br><span class="line">|      6|      86|   3|883603013|</span><br><span class="line">|     62|     257|   2|879372434|</span><br><span class="line">|    286|    1014|   5|879781125|</span><br><span class="line">|    200|     222|   5|876042340|</span><br><span class="line">|    210|      40|   3|891035994|</span><br><span class="line">|    224|      29|   3|888104457|</span><br><span class="line">|    303|     785|   3|879485318|</span><br><span class="line">|    122|     387|   5|879270459|</span><br><span class="line">|    194|     274|   2|879539794|</span><br><span class="line">|    291|    1042|   4|874834944|</span><br><span class="line">|    234|    1184|   2|892079237|</span><br><span class="line">+-------+--------+----+---------+</span><br><span class="line">only showing top 20 rows</span><br></pre></td></tr></table></figure>

<blockquote>
<p>注意：</p>
<ul>
<li><p>jdbc连接字符串中，建议使用 useSSL&#x3D;false 确保连接可以正常连接 （不使用SSL安全协议进行连接）</p>
</li>
<li><p>jdbc连接字符串中，建议使用 useUnicode&#x3D;true 来确保传输中不出现乱码</p>
</li>
<li><p>save()不要填参数，没有路径，是写出数据库</p>
</li>
<li><p>dbtable属性：指定写出的表名</p>
</li>
<li><p>读出来的是自带schema，不需要设置schema，因为数据库就有schema</p>
</li>
<li><p>load()不需要加参数，没有路径，从数据库汇总读取的</p>
</li>
<li><p>dbtable：是指定读取的表名</p>
</li>
</ul>
</blockquote>
<h3 id="3-3-10-总结"><a href="#3-3-10-总结" class="headerlink" title="3.3.10 总结"></a>3.3.10 总结</h3><blockquote>
<ol>
<li><p>DataFrame 在结构层面上由StructField组成列描述，由StructType构造表描述。在数据层面上，Column对象记录列数据，Row对象记录行数据</p>
</li>
<li><p>DataFrame可以从RDD转换、Pandas DF转换、读取文件、读取JDBC等方法构建</p>
</li>
<li><p><code>spark.read.format()</code>和<code>df.write.format()</code> 是DataFrame读取和写出的统一化标准API</p>
</li>
<li><p>SparkSQL默认在Shuffle阶段200个分区，可以修改参数获得最好性能</p>
</li>
<li><p><code>dropDuplicates</code>可以去重、<code>dropna</code>可以删除缺失值、<code>fillna</code>可以填充缺失值</p>
</li>
<li><p>SparkSQL支持JDBC读写，可用标准API对数据库进行读写操作</p>
</li>
</ol>
</blockquote>
<h2 id="3-4-SparkSQL函数定义"><a href="#3-4-SparkSQL函数定义" class="headerlink" title="3.4 SparkSQL函数定义"></a>3.4 SparkSQL函数定义</h2><h3 id="3-4-1-SparkSQL定义UDF函数"><a href="#3-4-1-SparkSQL定义UDF函数" class="headerlink" title="3.4.1 SparkSQL定义UDF函数"></a>3.4.1 SparkSQL定义UDF函数</h3><p>无论Hive还是SparkSQL分析处理数据时，往往需要使用函数，SparkSQL模块本身自带很多实现公共功能的函数，在<code>pyspark.sql.functions</code>中。SparkSQL与Hive一样支持定义函数:UDF和UDAF，尤其是<strong>UDF函数在实际项目中使用最为广泛</strong>。</p>
<p>回顾Hive中自定义函数有三种类型：</p>
<ul>
<li>第一种：UDF（User-Defined-Function）函数</li>
</ul>
<p>​	一对一的关系，输入一个值，经过函数以后输出一个值；</p>
<p>​	在Hive中继承UDF类，方法名称为evaluate,返回值不能为void，其实就是实现一个方法；</p>
<ul>
<li>第二种：UDAF（User-Defined Aggregation Function）聚合函数</li>
</ul>
<p>​	多对一的关系，输入个值输出一个值，通常与groupBy联合使用；</p>
<ul>
<li>第三种：UDTF（User-Defined Table-Generating Functions）函数</li>
</ul>
<p>​	一对多的关系，输入一个值输出多个值（一行变为多行）</p>
<p>​	用户自定义生成函数，有点像flatMap；</p>
<hr>
<p>目前来说Spark框架各个版本及各种语言对自定义函数的支持：</p>
<p><img src="https://cdn.jsdelivr.net/gh/Sql88/BlogImg@main/img/image-20231013153818332.png" alt="image-20231013153818332"></p>
<p>在SparkSQL中，目前仅仅支持UDF函数和UDAF函数。目前Python仅支持UDF。</p>
<hr>
<p>UDF定义方式有两种：</p>
<ol>
<li><code>sparksession.udf.register()</code>注册的UDF可以用于DSL和SQL，返回值用于DSL风格,传参内给的名字用于SQL风格</li>
<li><code>pyspark.sql.functions.udf</code>，仅能用于DSL风格</li>
</ol>
<p>代码案例：<code>16_udf_define.py</code></p>
<p>代码演示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding:utf8</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> functions <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> StructType, StringType, IntegerType</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># 0. 构建执行环境入口对象SparkSession</span></span><br><span class="line">    spark = SparkSession.builder. \</span><br><span class="line">        appName(<span class="string">&quot;16_udf_define.py&quot;</span>). \</span><br><span class="line">        master(<span class="string">&quot;local[*]&quot;</span>). \</span><br><span class="line">        getOrCreate()</span><br><span class="line">    sc = spark.sparkContext</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 构建一个RDD</span></span><br><span class="line">    rdd = sc.parallelize([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>]).<span class="built_in">map</span>(<span class="keyword">lambda</span> x: [x])</span><br><span class="line">    df = rdd.toDF([<span class="string">&quot;num&quot;</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># TODO 1:方式1 sparksession.udf.register()，DSL和SQL风格均可以使用</span></span><br><span class="line">    <span class="comment"># UDF的处理方法</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">num_multi_10</span>(<span class="params">num</span>):</span><br><span class="line">        <span class="keyword">return</span> num * <span class="number">10</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 参数1：注册的UDF名称，这个udf名称，仅可以用于SQL风格</span></span><br><span class="line">    <span class="comment"># 参数2：UDF的处理逻辑，是一个单独的方法</span></span><br><span class="line">    <span class="comment"># 参数3：声明UDF的返回值类型，注意：UDF注册时候，必须声明返回值类型，并且UDF的真实返回值一定要和声明的返回值一致</span></span><br><span class="line">    <span class="comment"># 返回值对象：这是一个UDF对象，仅可以用于DSL语法</span></span><br><span class="line">    <span class="comment"># 当前这种方式定义的UDF，可以通过参数1的名称用于SQL风格，通过返回值对象用于DSL风格</span></span><br><span class="line">    udf2 = spark.udf.register(<span class="string">&quot;udf1&quot;</span>, num_multi_10, IntegerType())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># SQL风格中使用</span></span><br><span class="line">    <span class="comment"># selectExpr 以SELECT的表达式执行，SQL风格的表达式（字符串）</span></span><br><span class="line">    <span class="comment"># select方法，接收普通的字符串字段名，或者返回值是Column对象的计算</span></span><br><span class="line">    df.selectExpr(<span class="string">&quot;udf1(num)&quot;</span>).show()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># DSL风格中使用</span></span><br><span class="line">    <span class="comment"># 返回值UDF对象 如果作为方法使用，传入的参数一定是Column对象</span></span><br><span class="line">    df.select(udf2(df[<span class="string">&#x27;num&#x27;</span>])).show()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># TODO 2:方式2注册，仅能用于DSL风格</span></span><br><span class="line">    udf3 = F.udf(num_multi_10,IntegerType())</span><br><span class="line">    df.select(udf3(df[<span class="string">&#x27;num&#x27;</span>])).show()</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>代码运行结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">+---------+</span><br><span class="line">|udf1(num)|</span><br><span class="line">+---------+</span><br><span class="line">|       10|</span><br><span class="line">|       20|</span><br><span class="line">|       30|</span><br><span class="line">|       40|</span><br><span class="line">|       50|</span><br><span class="line">|       60|</span><br><span class="line">|       70|</span><br><span class="line">+---------+</span><br><span class="line"></span><br><span class="line">+---------+</span><br><span class="line">|udf1(num)|</span><br><span class="line">+---------+</span><br><span class="line">|       10|</span><br><span class="line">|       20|</span><br><span class="line">|       30|</span><br><span class="line">|       40|</span><br><span class="line">|       50|</span><br><span class="line">|       60|</span><br><span class="line">|       70|</span><br><span class="line">+---------+</span><br><span class="line"></span><br><span class="line">+-----------------+</span><br><span class="line">|num_multi_10(num)|</span><br><span class="line">+-----------------+</span><br><span class="line">|               10|</span><br><span class="line">|               20|</span><br><span class="line">|               30|</span><br><span class="line">|               40|</span><br><span class="line">|               50|</span><br><span class="line">|               60|</span><br><span class="line">|               70|</span><br><span class="line">+-----------------+</span><br></pre></td></tr></table></figure>

<hr>
<p>注册一个返回值为ArrayType（数字\list）类型的UDF</p>
<p>代码案例：<code>17_udf_return_array.py</code></p>
<p>代码演示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding:utf8</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> functions <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> StructType, StringType, IntegerType, ArrayType</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># 0. 构建执行环境入口对象SparkSession</span></span><br><span class="line">    spark = SparkSession.builder. \</span><br><span class="line">        appName(<span class="string">&quot;17_udf_return_array.py&quot;</span>). \</span><br><span class="line">        master(<span class="string">&quot;local[*]&quot;</span>). \</span><br><span class="line">        getOrCreate()</span><br><span class="line">    sc = spark.sparkContext</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 构建一个RDD</span></span><br><span class="line">    rdd = sc.parallelize([[<span class="string">&quot;hadoop spark flink&quot;</span>], [<span class="string">&quot;hadoop flink java&quot;</span>]])</span><br><span class="line">    df = rdd.toDF([<span class="string">&quot;line&quot;</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 注册UDF，UDF的执行函数定义</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">split_line</span>(<span class="params">data</span>):</span><br><span class="line">        <span class="keyword">return</span> data.split(<span class="string">&quot; &quot;</span>)  <span class="comment"># 返回值是一个Array对象</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># TODO 1: 方式1 构建UDF</span></span><br><span class="line">    udf2 = spark.udf.register(<span class="string">&quot;udf1&quot;</span>, split_line, ArrayType(StringType()))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># DSL风格</span></span><br><span class="line">    df.select(udf2(df[<span class="string">&#x27;line&#x27;</span>])).show()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># SQL风格</span></span><br><span class="line">    df.createTempView(<span class="string">&quot;lines&quot;</span>)</span><br><span class="line">    spark.sql(<span class="string">&quot;SELECT udf1(line) FROM lines&quot;</span>).show(truncate=<span class="literal">False</span>)  <span class="comment"># truncate=False不切割字符串，全部显示出来</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># TODO 2：方式2的形式构建UDF</span></span><br><span class="line">    udf3 = F.udf(split_line, ArrayType(StringType()))</span><br><span class="line">    df.select(udf3(df[<span class="string">&#x27;line&#x27;</span>])).show(truncate=<span class="literal">False</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>运行结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">+--------------------+</span><br><span class="line">|          udf1(line)|</span><br><span class="line">+--------------------+</span><br><span class="line">|[hadoop, spark, f...|</span><br><span class="line">|[hadoop, flink, j...|</span><br><span class="line">+--------------------+</span><br><span class="line"></span><br><span class="line">+----------------------+</span><br><span class="line">|udf1(line)            |</span><br><span class="line">+----------------------+</span><br><span class="line">|[hadoop, spark, flink]|</span><br><span class="line">|[hadoop, flink, java] |</span><br><span class="line">+----------------------+</span><br><span class="line"></span><br><span class="line">+----------------------+</span><br><span class="line">|split_line(line)      |</span><br><span class="line">+----------------------+</span><br><span class="line">|[hadoop, spark, flink]|</span><br><span class="line">|[hadoop, flink, java] |</span><br><span class="line">+----------------------+</span><br></pre></td></tr></table></figure>

<blockquote>
<p>注意：数组或者list类型，可以使用spark的ArrayType来描述即可</p>
<p>声明ArrayType要类似这样：<code>ArrayType(StringType())</code>,在ArrayType中传入数组内的数据类型</p>
</blockquote>
<hr>
<p>注册一个返回值为字典类型的UDF</p>
<p>代码案例：<code>18_udf_return_dict.py</code></p>
<p>代码演示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding:utf8</span></span><br><span class="line"><span class="keyword">import</span> string</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> functions <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> StructType, StringType, IntegerType, ArrayType</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># 0. 构建执行环境入口对象SparkSession</span></span><br><span class="line">    spark = SparkSession.builder. \</span><br><span class="line">        appName(<span class="string">&quot;18_udf_return_dict.py&quot;</span>). \</span><br><span class="line">        master(<span class="string">&quot;local[*]&quot;</span>). \</span><br><span class="line">        getOrCreate()</span><br><span class="line">    sc = spark.sparkContext</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 假设 有三个数字 1 2 3 ，我们传入数字，返回数字所在序号对应的 字母 然后和数字结合形成dict返回</span></span><br><span class="line">    <span class="comment"># 比如传入1 我们返回&#123;&quot;num&quot;:1,&quot;letters&quot;:&quot;a&quot;&#125;</span></span><br><span class="line">    rdd = sc.parallelize([[<span class="number">1</span>], [<span class="number">2</span>], [<span class="number">3</span>]])</span><br><span class="line">    df = rdd.toDF([<span class="string">&quot;num&quot;</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 注册UDF</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process</span>(<span class="params">data</span>):</span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">&quot;num&quot;</span>: data, <span class="string">&quot;letters&quot;</span>: string.ascii_letters[data]&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    UDF的返回值是字典的话，需要用StructType来接收</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    udf1 = spark.udf.register(<span class="string">&quot;udf1&quot;</span>, process, StructType().add(<span class="string">&quot;num&quot;</span>, IntegerType(), nullable=<span class="literal">True</span>). \</span><br><span class="line">                              add(<span class="string">&quot;letters&quot;</span>, StringType(), nullable=<span class="literal">True</span>)</span><br><span class="line">                              )</span><br><span class="line">    df.selectExpr(<span class="string">&quot;udf1(num)&quot;</span>).show(truncate=<span class="literal">False</span>)</span><br><span class="line">    df.select(udf1(df[<span class="string">&#x27;num&#x27;</span>])).show(truncate=<span class="literal">False</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>运行结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">+---------+</span><br><span class="line">|udf1(num)|</span><br><span class="line">+---------+</span><br><span class="line">|&#123;1, b&#125;   |</span><br><span class="line">|&#123;2, c&#125;   |</span><br><span class="line">|&#123;3, d&#125;   |</span><br><span class="line">+---------+</span><br><span class="line"></span><br><span class="line">+---------+</span><br><span class="line">|udf1(num)|</span><br><span class="line">+---------+</span><br><span class="line">|&#123;1, b&#125;   |</span><br><span class="line">|&#123;2, c&#125;   |</span><br><span class="line">|&#123;3, d&#125;   |</span><br><span class="line">+---------+</span><br></pre></td></tr></table></figure>

<blockquote>
<p>注意：字典类型返回值，可以用StructType来进行描述</p>
<p>StructType是一个普通的Spark支持的结构化类型</p>
<p>只是可以用在：</p>
<ul>
<li>DF中用于描述Schema</li>
<li>UDF中用于描述返回值是字典的数据</li>
</ul>
</blockquote>
<hr>
<p>用mapPartitions API 完成UDAF聚合</p>
<p>代码案例：<code>19_udaf_by_rdd.py</code></p>
<p>代码演示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding:utf8</span></span><br><span class="line"><span class="keyword">import</span> string</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> functions <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> StructType, StringType, IntegerType, ArrayType</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># 0. 构建执行环境入口对象SparkSession</span></span><br><span class="line">    spark = SparkSession.builder. \</span><br><span class="line">        appName(<span class="string">&quot;19_udaf_by_rdd.py&quot;</span>). \</span><br><span class="line">        master(<span class="string">&quot;local[*]&quot;</span>). \</span><br><span class="line">        getOrCreate()</span><br><span class="line">    sc = spark.sparkContext</span><br><span class="line"></span><br><span class="line">    rdd = sc.parallelize([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>], <span class="number">3</span>)</span><br><span class="line">    df = rdd.<span class="built_in">map</span>(<span class="keyword">lambda</span> x: [x]).toDF([<span class="string">&#x27;num&#x27;</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 折中的方式 就是使用RDD的mapPartitions 算子来完成聚合操作</span></span><br><span class="line">    <span class="comment"># 如果用mapPartitions API 完成UDAF聚合，一定要单分区</span></span><br><span class="line">    single_partition_rdd = df.rdd.repartition(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># print(single_partition_rdd.collect())</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process</span>(<span class="params"><span class="built_in">iter</span></span>):</span><br><span class="line">        <span class="built_in">sum</span> = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> row <span class="keyword">in</span> <span class="built_in">iter</span>:</span><br><span class="line">            <span class="built_in">sum</span> += row[<span class="string">&#x27;num&#x27;</span>]</span><br><span class="line">        <span class="keyword">return</span> [<span class="built_in">sum</span>]  <span class="comment"># 一定要嵌套list，因为mapPartitions方法要求的返回值是list对象</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(single_partition_rdd.mapPartitions(process).collect())</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>运行结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[15]</span><br></pre></td></tr></table></figure>

<blockquote>
<p>注意：</p>
<p>使用UDF两种方式的注册均可以。</p>
<p>唯一需要注意的就是：返回值类型一定要有合适的类型来声明</p>
<p>返回 int 可以用IntergerType</p>
<p>返回小数，可以用FloatType或者DoubleType</p>
<p>返回数组list 可以用ArrayType描述</p>
<p>返回字典 可以用StructType描述</p>
<p>…..</p>
<p>这些Spark内置的数据类型均存储在：</p>
<p><code>pyspark.sql.types</code>包中。</p>
</blockquote>
<h3 id="3-4-2-SparkSQL使用函数窗口"><a href="#3-4-2-SparkSQL使用函数窗口" class="headerlink" title="3.4.2 SparkSQL使用函数窗口"></a>3.4.2 SparkSQL使用函数窗口</h3><p><strong>开窗函数</strong>：</p>
<p>开窗函数的引入是为了既显示聚集前的数据，又显示聚集后的数据。即在每一行的最后一列添加聚合函数的结果。<br>开窗用于为行定义一个窗口(这里的窗口是指运算将要操作的行的集合)，它对一组值进行操作，不需要使用GROUP BY子句对数据进行分组，能够在同一行中同时返回基础行的列和聚合列。</p>
<p><strong>窗口函数的语法：</strong></p>
<p><img src="https://cdn.jsdelivr.net/gh/Sql88/BlogImg@main/img/image-20231110144853033.png" alt="image-20231110144853033"></p>
<p>代码演示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding:utf8</span></span><br><span class="line"><span class="keyword">import</span> string</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> functions <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> StructType, StringType, IntegerType, ArrayType</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># 0. 构建执行环境入口对象SparkSession</span></span><br><span class="line">    spark = SparkSession.builder. \</span><br><span class="line">        appName(<span class="string">&quot;20_window_function.py&quot;</span>). \</span><br><span class="line">        master(<span class="string">&quot;local[*]&quot;</span>). \</span><br><span class="line">        getOrCreate()</span><br><span class="line">    sc = spark.sparkContext</span><br><span class="line"></span><br><span class="line">    rdd = sc.parallelize([</span><br><span class="line">        (<span class="string">&#x27;张1&#x27;</span>, <span class="string">&#x27;class_1&#x27;</span>, <span class="number">99</span>),</span><br><span class="line">        (<span class="string">&#x27;张2&#x27;</span>, <span class="string">&#x27;class_2&#x27;</span>, <span class="number">35</span>),</span><br><span class="line">        (<span class="string">&#x27;张3&#x27;</span>, <span class="string">&#x27;class_3&#x27;</span>, <span class="number">57</span>),</span><br><span class="line">        (<span class="string">&#x27;张4&#x27;</span>, <span class="string">&#x27;class_4&#x27;</span>, <span class="number">12</span>),</span><br><span class="line">        (<span class="string">&#x27;张5&#x27;</span>, <span class="string">&#x27;class_5&#x27;</span>, <span class="number">99</span>),</span><br><span class="line">        (<span class="string">&#x27;张6&#x27;</span>, <span class="string">&#x27;class_1&#x27;</span>, <span class="number">90</span>),</span><br><span class="line">        (<span class="string">&#x27;张7&#x27;</span>, <span class="string">&#x27;class_2&#x27;</span>, <span class="number">91</span>),</span><br><span class="line">        (<span class="string">&#x27;张8&#x27;</span>, <span class="string">&#x27;class_3&#x27;</span>, <span class="number">33</span>),</span><br><span class="line">        (<span class="string">&#x27;张9&#x27;</span>, <span class="string">&#x27;class_4&#x27;</span>, <span class="number">55</span>),</span><br><span class="line">        (<span class="string">&#x27;张10&#x27;</span>, <span class="string">&#x27;class_5&#x27;</span>, <span class="number">66</span>),</span><br><span class="line">        (<span class="string">&#x27;张11&#x27;</span>, <span class="string">&#x27;class_1&#x27;</span>, <span class="number">11</span>),</span><br><span class="line">        (<span class="string">&#x27;张12&#x27;</span>, <span class="string">&#x27;class_2&#x27;</span>, <span class="number">33</span>),</span><br><span class="line">        (<span class="string">&#x27;张13&#x27;</span>, <span class="string">&#x27;class_3&#x27;</span>, <span class="number">36</span>),</span><br><span class="line">        (<span class="string">&#x27;张14&#x27;</span>, <span class="string">&#x27;class_4&#x27;</span>, <span class="number">79</span>),</span><br><span class="line">        (<span class="string">&#x27;张15&#x27;</span>, <span class="string">&#x27;class_5&#x27;</span>, <span class="number">90</span>),</span><br><span class="line">        (<span class="string">&#x27;张16&#x27;</span>, <span class="string">&#x27;class_1&#x27;</span>, <span class="number">90</span>),</span><br><span class="line">        (<span class="string">&#x27;张17&#x27;</span>, <span class="string">&#x27;class_2&#x27;</span>, <span class="number">90</span>),</span><br><span class="line">        (<span class="string">&#x27;张18&#x27;</span>, <span class="string">&#x27;class_3&#x27;</span>, <span class="number">11</span>),</span><br><span class="line">        (<span class="string">&#x27;张19&#x27;</span>, <span class="string">&#x27;class_1&#x27;</span>, <span class="number">11</span>),</span><br><span class="line">        (<span class="string">&#x27;张20&#x27;</span>, <span class="string">&#x27;class_2&#x27;</span>, <span class="number">3</span>),</span><br><span class="line">        (<span class="string">&#x27;张21&#x27;</span>, <span class="string">&#x27;class_3&#x27;</span>, <span class="number">99</span>),</span><br><span class="line">    ])</span><br><span class="line">    schema = StructType().add(<span class="string">&quot;name&quot;</span>, StringType()). \</span><br><span class="line">        add(<span class="string">&quot;class&quot;</span>, StringType()). \</span><br><span class="line">        add(<span class="string">&quot;score&quot;</span>, IntegerType())</span><br><span class="line"></span><br><span class="line">    df = rdd.toDF(schema)</span><br><span class="line">    df.createTempView(<span class="string">&quot;stu&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># TODO 聚合窗口函数的演示</span></span><br><span class="line">    spark.sql(<span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        select *,avg(score) over() as avg_score from stu</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span>).show()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># TODO 排序相关的窗口函数计算</span></span><br><span class="line">    <span class="comment"># RANK over，DENSE_RANK over ROW_NUMBER over</span></span><br><span class="line">    spark.sql(<span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        SELECT *,ROW_NUMBER() OVER(ORDER BY score DESC) AS row_number_rank,</span></span><br><span class="line"><span class="string">         DENSE_RANK() OVER(PARTITION BY class ORDER BY score DESC) AS dense_rank,</span></span><br><span class="line"><span class="string">         RANK() OVER(ORDER BY score) AS rank </span></span><br><span class="line"><span class="string">         FROM stu</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span>).show()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># TODO NTILE</span></span><br><span class="line">    spark.sql(<span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        select * ,NTILE(6) OVER(ORDER BY score desc) AS NTILE_result from stu</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span>).show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>输出结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line">+----+-------+-----+------------------+</span><br><span class="line">|name|  class|score|         avg_score|</span><br><span class="line">+----+-------+-----+------------------+</span><br><span class="line">| 张1|class_1|   99|56.666666666666664|</span><br><span class="line">| 张2|class_2|   35|56.666666666666664|</span><br><span class="line">| 张3|class_3|   57|56.666666666666664|</span><br><span class="line">| 张4|class_4|   12|56.666666666666664|</span><br><span class="line">| 张5|class_5|   99|56.666666666666664|</span><br><span class="line">| 张6|class_1|   90|56.666666666666664|</span><br><span class="line">| 张7|class_2|   91|56.666666666666664|</span><br><span class="line">| 张8|class_3|   33|56.666666666666664|</span><br><span class="line">| 张9|class_4|   55|56.666666666666664|</span><br><span class="line">|张10|class_5|   66|56.666666666666664|</span><br><span class="line">|张11|class_1|   11|56.666666666666664|</span><br><span class="line">|张12|class_2|   33|56.666666666666664|</span><br><span class="line">|张13|class_3|   36|56.666666666666664|</span><br><span class="line">|张14|class_4|   79|56.666666666666664|</span><br><span class="line">|张15|class_5|   90|56.666666666666664|</span><br><span class="line">|张16|class_1|   90|56.666666666666664|</span><br><span class="line">|张17|class_2|   90|56.666666666666664|</span><br><span class="line">|张18|class_3|   11|56.666666666666664|</span><br><span class="line">|张19|class_1|   11|56.666666666666664|</span><br><span class="line">|张20|class_2|    3|56.666666666666664|</span><br><span class="line">+----+-------+-----+------------------+</span><br><span class="line">only showing top 20 rows</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">+----+-------+-----+---------------+----------+----+</span><br><span class="line">|name|  class|score|row_number_rank|dense_rank|rank|</span><br><span class="line">+----+-------+-----+---------------+----------+----+</span><br><span class="line">| 张1|class_1|   99|              1|         1|  19|</span><br><span class="line">| 张6|class_1|   90|              5|         2|  14|</span><br><span class="line">|张16|class_1|   90|              7|         2|  14|</span><br><span class="line">|张11|class_1|   11|             18|         3|   2|</span><br><span class="line">|张19|class_1|   11|             20|         3|   2|</span><br><span class="line">| 张7|class_2|   91|              4|         1|  18|</span><br><span class="line">|张17|class_2|   90|              8|         2|  14|</span><br><span class="line">| 张2|class_2|   35|             14|         3|   8|</span><br><span class="line">|张12|class_2|   33|             16|         4|   6|</span><br><span class="line">|张20|class_2|    3|             21|         5|   1|</span><br><span class="line">|张21|class_3|   99|              3|         1|  19|</span><br><span class="line">| 张3|class_3|   57|             11|         2|  11|</span><br><span class="line">|张13|class_3|   36|             13|         3|   9|</span><br><span class="line">| 张8|class_3|   33|             15|         4|   6|</span><br><span class="line">|张18|class_3|   11|             19|         5|   2|</span><br><span class="line">|张14|class_4|   79|              9|         1|  13|</span><br><span class="line">| 张9|class_4|   55|             12|         2|  10|</span><br><span class="line">| 张4|class_4|   12|             17|         3|   5|</span><br><span class="line">| 张5|class_5|   99|              2|         1|  19|</span><br><span class="line">|张15|class_5|   90|              6|         2|  14|</span><br><span class="line">+----+-------+-----+---------------+----------+----+</span><br><span class="line">only showing top 20 rows</span><br><span class="line"></span><br><span class="line">+----+-------+-----+------------+</span><br><span class="line">|name|  class|score|NTILE_result|</span><br><span class="line">+----+-------+-----+------------+</span><br><span class="line">| 张1|class_1|   99|           1|</span><br><span class="line">| 张5|class_5|   99|           1|</span><br><span class="line">|张21|class_3|   99|           1|</span><br><span class="line">| 张7|class_2|   91|           1|</span><br><span class="line">| 张6|class_1|   90|           2|</span><br><span class="line">|张15|class_5|   90|           2|</span><br><span class="line">|张16|class_1|   90|           2|</span><br><span class="line">|张17|class_2|   90|           2|</span><br><span class="line">|张14|class_4|   79|           3|</span><br><span class="line">|张10|class_5|   66|           3|</span><br><span class="line">| 张3|class_3|   57|           3|</span><br><span class="line">| 张9|class_4|   55|           3|</span><br><span class="line">|张13|class_3|   36|           4|</span><br><span class="line">| 张2|class_2|   35|           4|</span><br><span class="line">| 张8|class_3|   33|           4|</span><br><span class="line">|张12|class_2|   33|           5|</span><br><span class="line">| 张4|class_4|   12|           5|</span><br><span class="line">|张11|class_1|   11|           5|</span><br><span class="line">|张18|class_3|   11|           6|</span><br><span class="line">|张19|class_1|   11|           6|</span><br><span class="line">+----+-------+-----+------------+</span><br></pre></td></tr></table></figure>

<h3 id="3-4-3-总结"><a href="#3-4-3-总结" class="headerlink" title="3.4.3 总结"></a>3.4.3 总结</h3><blockquote>
<ol>
<li>SparkSQL支持UDF和UDAF定义，但在Python中，暂时只能定义UDF</li>
<li>UDF定义支持2种方式，1：使用SparkSession对象构建。2：使用functions包中提供的UDF API构建。要注意，方式1可用DSL和SQL风格，方式2可用于DSL风格。</li>
<li>SparkSQL支持窗口函数使用，常用SQL中的窗口函数均支持，如聚合窗口、排序窗口、NTILE分组窗口等。</li>
</ol>
</blockquote>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="https://sql88.github.io">靓仔阿胜</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://sql88.github.io/post/1e5a3fd5.html">https://sql88.github.io/post/1e5a3fd5.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://sql88.github.io" target="_blank">靓仔阿胜的博客</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/SparkSQL/">SparkSQL</a></div><div class="post_share"><div class="social-share" data-image="/img/avatar1.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i> 打赏</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="/img/wechat.png" target="_blank"><img class="post-qr-code-img" src="/img/wechat.png" alt="wechat"/></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="/img/alipay.jpg" target="_blank"><img class="post-qr-code-img" src="/img/alipay.jpg" alt="alipay"/></a><div class="post-qr-code-desc">alipay</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/post/ece8daf4.html" title="PySpark-核心编程"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">PySpark-核心编程</div></div></a></div><div class="next-post pull-right"><a href="/post/d891eb20.html" title="常用破解软件"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">常用破解软件</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/avatar1.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">靓仔阿胜</div><div class="author-info__description">SqlBoy 的天堂</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">45</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">37</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">21</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/Sql88/Sql88.github.io"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://githubfast.com/Sql88" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="https://blog.csdn.net/qq_42577508" target="_blank" title="CSDN"><i class="fa fa-book-open"></i></a><a class="social-icon" href="tencent://AddContact/?fromId=45&amp;fromSubId=1&amp;subcmd=all&amp;uin=1131977233&amp;website=www.oicqzone.com" target="_blank" title="QQ"><i class="fab fa-qq"></i></a><a class="social-icon" href="mailto:1131977233@qq.com" target="_blank" title="Email"><i class="fas fa-envelope-open-text"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">I am a SQlBoy</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#3-PySpark-SparkSQL"><span class="toc-text">3.PySpark-SparkSQL</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#3-1-SparkSQL%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8"><span class="toc-text">3.1 SparkSQL快速入门</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-1-%E4%BB%80%E4%B9%88%E6%98%AFSparkSQL"><span class="toc-text">3.1.1 什么是SparkSQL</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-2-%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E5%AD%A6%E4%B9%A0SparkSQL"><span class="toc-text">3.1.2 为什么要学习SparkSQL</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-3-SparkSQL%E7%89%B9%E7%82%B9"><span class="toc-text">3.1.3 SparkSQL特点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-4-SparkSQL%E5%8F%91%E5%B1%95%E5%8F%B2"><span class="toc-text">3.1.4 SparkSQL发展史</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-5-%E6%80%BB%E7%BB%93"><span class="toc-text">3.1.5 总结</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-2-SparkSQL%E6%A6%82%E8%BF%B0"><span class="toc-text">3.2 SparkSQL概述</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-1-SparkSQL%E5%92%8CHive%E7%9A%84%E5%BC%82%E5%90%8C"><span class="toc-text">3.2.1 SparkSQL和Hive的异同</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-2-SparkSQL%E7%9A%84%E6%95%B0%E6%8D%AE%E6%8A%BD%E8%B1%A1"><span class="toc-text">3.2.2 SparkSQL的数据抽象</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-3-SparkSQL%E6%95%B0%E6%8D%AE%E6%8A%BD%E8%B1%A1%E7%9A%84%E5%8F%91%E5%B1%95"><span class="toc-text">3.2.3 SparkSQL数据抽象的发展</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-4-DataFrame%E6%A6%82%E8%BF%B0"><span class="toc-text">3.2.4 DataFrame概述</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-5-SparkSession%E5%AF%B9%E8%B1%A1"><span class="toc-text">3.2.5 SparkSession对象</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-6-SparkSQL-HelloWorld"><span class="toc-text">3.2.6 SparkSQL HelloWorld</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-7-%E6%80%BB%E7%BB%93"><span class="toc-text">3.2.7 总结</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-3-DataFrame%E5%85%A5%E9%97%A8"><span class="toc-text">3.3 DataFrame入门</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-1-DataFrame%E7%9A%84%E7%BB%84%E6%88%90"><span class="toc-text">3.3.1 DataFrame的组成</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-2-DataFrame%E7%9A%84%E4%BB%A3%E7%A0%81%E6%9E%84%E5%BB%BA"><span class="toc-text">3.3.2 DataFrame的代码构建</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-3-2-1-%E5%9F%BA%E4%BA%8ERDD%E6%96%B9%E5%BC%8F1"><span class="toc-text">3.3.2.1 基于RDD方式1</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-3-2-2%E5%9F%BA%E4%BA%8ERDD%E6%96%B9%E5%BC%8F2"><span class="toc-text">3.3.2.2基于RDD方式2</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-3-2-3-%E5%9F%BA%E4%BA%8ERDD%E6%96%B9%E5%BC%8F3"><span class="toc-text">3.3.2.3 基于RDD方式3</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-3-2-4-%E5%9F%BA%E4%BA%8EPandas%E7%9A%84DF"><span class="toc-text">3.3.2.4 基于Pandas的DF</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-3-2-5-%E8%AF%BB%E5%8F%96%E5%A4%96%E9%83%A8%E6%95%B0%E6%8D%AE"><span class="toc-text">3.3.2.5 读取外部数据</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-3-DataFrame%E7%9A%84%E5%85%A5%E9%97%A8%E6%93%8D%E4%BD%9C"><span class="toc-text">3.3.3 DataFrame的入门操作</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-3-3-1-DSL%E9%A3%8E%E6%A0%BC"><span class="toc-text">3.3.3.1 DSL风格</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-3-3-2-SQL%E9%A3%8E%E6%A0%BC"><span class="toc-text">3.3.3.2 SQL风格</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-4-%E8%AF%8D%E9%A2%91%E7%BB%9F%E8%AE%A1%E6%A1%88%E4%BE%8B"><span class="toc-text">3.3.4 词频统计案例</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-5-%E7%94%B5%E5%BD%B1%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90"><span class="toc-text">3.5 电影数据分析</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-6-SparkSQL-Shuffle-%E5%88%86%E5%8C%BA%E6%95%B0%E7%9B%AE"><span class="toc-text">3.3.6 SparkSQL Shuffle 分区数目</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-7-SparkSQL-%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97API"><span class="toc-text">3.3.7 SparkSQL 数据清洗API</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-8-DataFrame%E6%95%B0%E6%8D%AE%E5%86%99%E5%87%BA"><span class="toc-text">3.3.8 DataFrame数据写出</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-9-DataFrame-%E9%80%9A%E8%BF%87JDBC%E8%AF%BB%E5%86%99%E6%95%B0%E6%8D%AE%E5%BA%93%EF%BC%88MySQL%E7%A4%BA%E4%BE%8B%EF%BC%89"><span class="toc-text">3.3.9 DataFrame 通过JDBC读写数据库（MySQL示例）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-10-%E6%80%BB%E7%BB%93"><span class="toc-text">3.3.10 总结</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-4-SparkSQL%E5%87%BD%E6%95%B0%E5%AE%9A%E4%B9%89"><span class="toc-text">3.4 SparkSQL函数定义</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-1-SparkSQL%E5%AE%9A%E4%B9%89UDF%E5%87%BD%E6%95%B0"><span class="toc-text">3.4.1 SparkSQL定义UDF函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-2-SparkSQL%E4%BD%BF%E7%94%A8%E5%87%BD%E6%95%B0%E7%AA%97%E5%8F%A3"><span class="toc-text">3.4.2 SparkSQL使用函数窗口</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-3-%E6%80%BB%E7%BB%93"><span class="toc-text">3.4.3 总结</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/post/609b9ff3.html" title="Hadoop面试八股文">Hadoop面试八股文</a><time datetime="2024-06-25T03:34:00.000Z" title="发表于 2024-06-25 11:34:00">2024-06-25</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/post/14fac2ec.html" title="Flink 基础教程">Flink 基础教程</a><time datetime="2024-06-22T04:07:00.000Z" title="发表于 2024-06-22 12:07:00">2024-06-22</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/post/ba08a12e.html" title="数据仓库基础教程">数据仓库基础教程</a><time datetime="2024-06-21T06:17:00.000Z" title="发表于 2024-06-21 14:17:00">2024-06-21</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/post/9eea3e37.html" title="Hive基础教程">Hive基础教程</a><time datetime="2024-06-21T06:17:00.000Z" title="发表于 2024-06-21 14:17:00">2024-06-21</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/post/547b63bc.html" title="Zookeeper基础教程">Zookeeper基础教程</a><time datetime="2024-06-20T13:47:00.000Z" title="发表于 2024-06-20 21:47:00">2024-06-20</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('/img/background1.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2024 By 靓仔阿胜</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="chat-btn" type="button" title="聊天"><i class="fas fa-sms"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>(() => {
  const $mermaid = document.querySelectorAll('#article-container .mermaid-wrap')
  if ($mermaid.length === 0) return
  const runMermaid = () => {
    window.loadMermaid = true
    const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'default'

    Array.from($mermaid).forEach((item, index) => {
      const mermaidSrc = item.firstElementChild
      const mermaidThemeConfig = '%%{init:{ \'theme\':\'' + theme + '\'}}%%\n'
      const mermaidID = 'mermaid-' + index
      const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent

      const renderFn = mermaid.render(mermaidID, mermaidDefinition)

      const renderV10 = () => {
        renderFn.then(({svg}) => {
          mermaidSrc.insertAdjacentHTML('afterend', svg)
        })
      }

      const renderV9 = svg => {
        mermaidSrc.insertAdjacentHTML('afterend', svg)
      }

      typeof renderFn === 'string' ? renderV9(renderFn) : renderV10()
    })
  }

  const loadMermaid = () => {
    window.loadMermaid ? runMermaid() : getScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js').then(runMermaid)
  }

  btf.addModeChange('mermaid', runMermaid)

  window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
})()</script><script>(() => {
  function abcjsInit() {
    function abcjsFn() {
      for (let abcContainer of document.getElementsByClassName("abc-music-sheet")) {
        ABCJS.renderAbc(abcContainer, abcContainer.innerHTML, {responsive: 'resize'})
      }
    }
    
    typeof ABCJS === 'object' ? abcjsFn()
      : getScript('https://cdn.jsdelivr.net/npm/abcjs/dist/abcjs-basic-min.min.js').then(abcjsFn)
  }

  window.pjax ? abcjsInit() : document.addEventListener('DOMContentLoaded', abcjsInit)
})()</script></div><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-nest.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = true;
POWERMODE.mobile = true;
document.body.addEventListener('input', POWERMODE);
</script><script id="click-show-text" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-show-text.min.js" data-mobile="true" data-text="富强,民主,文明,和谐,自由,平等,公正,法治,爱国,敬业,诚信,友善" data-fontsize="15px" data-random="false" async="async"></script><script src="https://cdn.jsdelivr.net/npm/pjax/pjax.min.js"></script><script>let pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]

var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {

  // removeEventListener scroll 
  window.tocScrollFn && window.removeEventListener('scroll', window.tocScrollFn)
  window.scrollCollect && window.removeEventListener('scroll', scrollCollect)

  document.getElementById('rightside').style.cssText = "opacity: ''; transform: ''"
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')

  typeof disqusjs === 'object' && disqusjs.destroy()
})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()
})

document.addEventListener('pjax:error', (e) => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div class="no-result" id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div></body></html>